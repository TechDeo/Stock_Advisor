{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lRVShIAOEYZz"
   },
   "source": [
    "# Finacial Chatbot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yPNnHXZ1EYde"
   },
   "source": [
    "## Data Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "tU10UUjmDUga",
    "outputId": "86c8b1e1-014a-487a-ba34-555fe402224c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: yfinance in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (0.2.46)\n",
      "Requirement already satisfied: pandas>=1.3.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from yfinance) (2.1.4)\n",
      "Requirement already satisfied: numpy>=1.16.5 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from yfinance) (1.26.4)\n",
      "Requirement already satisfied: requests>=2.31 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from yfinance) (2.32.3)\n",
      "Requirement already satisfied: multitasking>=0.0.7 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from yfinance) (0.0.11)\n",
      "Requirement already satisfied: lxml>=4.9.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from yfinance) (5.3.0)\n",
      "Requirement already satisfied: platformdirs>=2.0.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from yfinance) (4.2.2)\n",
      "Requirement already satisfied: pytz>=2022.5 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from yfinance) (2024.1)\n",
      "Requirement already satisfied: frozendict>=2.3.4 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from yfinance) (2.4.6)\n",
      "Requirement already satisfied: peewee>=3.16.2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from yfinance) (3.17.7)\n",
      "Requirement already satisfied: beautifulsoup4>=4.11.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from yfinance) (4.12.3)\n",
      "Requirement already satisfied: html5lib>=1.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from yfinance) (1.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from beautifulsoup4>=4.11.1->yfinance) (2.6)\n",
      "Requirement already satisfied: six>=1.9 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from html5lib>=1.1->yfinance) (1.16.0)\n",
      "Requirement already satisfied: webencodings in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from html5lib>=1.1->yfinance) (0.5.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from pandas>=1.3.0->yfinance) (2.9.0.post0)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from pandas>=1.3.0->yfinance) (2024.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests>=2.31->yfinance) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests>=2.31->yfinance) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests>=2.31->yfinance) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests>=2.31->yfinance) (2024.8.30)\n"
     ]
    }
   ],
   "source": [
    "!pip install yfinance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "boayRod0DWQH"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import yfinance as yf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "F5TDaN69D_dy",
    "outputId": "fdc081d7-0bf6-464f-ce00-4b637fd0bb54"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [(Adj Close, AAPL), (Close, AAPL), (High, AAPL), (Low, AAPL), (Open, AAPL), (Volume, AAPL), (Ticker, ), (Adj Close, MSFT), (Close, MSFT), (High, MSFT), (Low, MSFT), (Open, MSFT), (Volume, MSFT), (Adj Close, GOOG), (Close, GOOG), (High, GOOG), (Low, GOOG), (Open, GOOG), (Volume, GOOG), (Adj Close, AMZN), (Close, AMZN), (High, AMZN), (Low, AMZN), (Open, AMZN), (Volume, AMZN), (Adj Close, TSLA), (Close, TSLA), (High, TSLA), (Low, TSLA), (Open, TSLA), (Volume, TSLA), (Adj Close, NVDA), (Close, NVDA), (High, NVDA), (Low, NVDA), (Open, NVDA), (Volume, NVDA), (Adj Close, BRK-B), (Close, BRK-B), (High, BRK-B), (Low, BRK-B), (Open, BRK-B), (Volume, BRK-B), (Adj Close, META), (Close, META), (High, META), (Low, META), (Open, META), (Volume, META), (Adj Close, V), (Close, V), (High, V), (Low, V), (Open, V), (Volume, V), (Adj Close, JNJ), (Close, JNJ), (High, JNJ), (Low, JNJ), (Open, JNJ), (Volume, JNJ)]\n",
      "Index: []\n",
      "\n",
      "[0 rows x 61 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "\n",
    "# List of 10 top stocks by market capitalization (commonly large companies)\n",
    "top_10_tickers = [\n",
    "    \"AAPL\",  # Apple\n",
    "    \"MSFT\",  # Microsoft\n",
    "    \"GOOG\",  # Alphabet (Google)\n",
    "    \"AMZN\",  # Amazon\n",
    "    \"TSLA\",  # Tesla\n",
    "    \"NVDA\",  # Nvidia\n",
    "    \"BRK-B\", # Berkshire Hathaway\n",
    "    \"META\",  # Meta (Facebook)\n",
    "    \"V\",     # Visa\n",
    "    \"JNJ\"    # Johnson & Johnson\n",
    "]\n",
    "\n",
    "# Fetch stock data for each ticker\n",
    "df = pd.DataFrame()\n",
    "for ticker in top_10_tickers:\n",
    "    stock_data = yf.download(ticker, start=\"2023-01-01\", end=\"2024-01-01\")\n",
    "    stock_data['Ticker'] = ticker\n",
    "    df = pd.concat([df, stock_data])\n",
    "    df.dropna(inplace=True)\n",
    "# Set the index as a combination of Date and Ticker for easy querying\n",
    "# df = df.reset_index().set_index(['Date', 'Ticker'])\n",
    "\n",
    "# Display the first few rows of the combined DataFrame\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "v2a1DKwgEC5Y",
    "outputId": "055592f9-bb9f-4bdf-e004-c3c1dcfd8893"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 61)\n"
     ]
    }
   ],
   "source": [
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 257
    },
    "id": "PpfDNBzZGjal",
    "outputId": "f2926046-f078-4234-c24d-e3f0565f37a9"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th>Price</th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Close</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Open</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Ticker</th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Close</th>\n",
       "      <th colspan=\"3\" halign=\"left\">High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Open</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Close</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Open</th>\n",
       "      <th>Volume</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ticker</th>\n",
       "      <th>AAPL</th>\n",
       "      <th>AAPL</th>\n",
       "      <th>AAPL</th>\n",
       "      <th>AAPL</th>\n",
       "      <th>AAPL</th>\n",
       "      <th>AAPL</th>\n",
       "      <th></th>\n",
       "      <th>MSFT</th>\n",
       "      <th>MSFT</th>\n",
       "      <th>MSFT</th>\n",
       "      <th>...</th>\n",
       "      <th>V</th>\n",
       "      <th>V</th>\n",
       "      <th>V</th>\n",
       "      <th>V</th>\n",
       "      <th>JNJ</th>\n",
       "      <th>JNJ</th>\n",
       "      <th>JNJ</th>\n",
       "      <th>JNJ</th>\n",
       "      <th>JNJ</th>\n",
       "      <th>JNJ</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>0 rows Ã— 61 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [(Adj Close, AAPL), (Close, AAPL), (High, AAPL), (Low, AAPL), (Open, AAPL), (Volume, AAPL), (Ticker, ), (Adj Close, MSFT), (Close, MSFT), (High, MSFT), (Low, MSFT), (Open, MSFT), (Volume, MSFT), (Adj Close, GOOG), (Close, GOOG), (High, GOOG), (Low, GOOG), (Open, GOOG), (Volume, GOOG), (Adj Close, AMZN), (Close, AMZN), (High, AMZN), (Low, AMZN), (Open, AMZN), (Volume, AMZN), (Adj Close, TSLA), (Close, TSLA), (High, TSLA), (Low, TSLA), (Open, TSLA), (Volume, TSLA), (Adj Close, NVDA), (Close, NVDA), (High, NVDA), (Low, NVDA), (Open, NVDA), (Volume, NVDA), (Adj Close, BRK-B), (Close, BRK-B), (High, BRK-B), (Low, BRK-B), (Open, BRK-B), (Volume, BRK-B), (Adj Close, META), (Close, META), (High, META), (Low, META), (Open, META), (Volume, META), (Adj Close, V), (Close, V), (High, V), (Low, V), (Open, V), (Volume, V), (Adj Close, JNJ), (Close, JNJ), (High, JNJ), (Low, JNJ), (Open, JNJ), (Volume, JNJ)]\n",
       "Index: []\n",
       "\n",
       "[0 rows x 61 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "ow3x3T0eOztk"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1224/1676923224.py:1: PerformanceWarning: dropping on a non-lexsorted multi-index without a level parameter may impact performance.\n",
      "  df.drop(columns=['Adj Close'], inplace=True)\n"
     ]
    }
   ],
   "source": [
    "df.drop(columns=['Adj Close'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wh74ZiHnO_7c",
    "outputId": "67911807-0185-4bb4-ecf0-e5df6c709d17"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultiIndex([( 'Close',  'AAPL'),\n",
       "            (  'High',  'AAPL'),\n",
       "            (   'Low',  'AAPL'),\n",
       "            (  'Open',  'AAPL'),\n",
       "            ('Volume',  'AAPL'),\n",
       "            ('Ticker',      ''),\n",
       "            ( 'Close',  'MSFT'),\n",
       "            (  'High',  'MSFT'),\n",
       "            (   'Low',  'MSFT'),\n",
       "            (  'Open',  'MSFT'),\n",
       "            ('Volume',  'MSFT'),\n",
       "            ( 'Close',  'GOOG'),\n",
       "            (  'High',  'GOOG'),\n",
       "            (   'Low',  'GOOG'),\n",
       "            (  'Open',  'GOOG'),\n",
       "            ('Volume',  'GOOG'),\n",
       "            ( 'Close',  'AMZN'),\n",
       "            (  'High',  'AMZN'),\n",
       "            (   'Low',  'AMZN'),\n",
       "            (  'Open',  'AMZN'),\n",
       "            ('Volume',  'AMZN'),\n",
       "            ( 'Close',  'TSLA'),\n",
       "            (  'High',  'TSLA'),\n",
       "            (   'Low',  'TSLA'),\n",
       "            (  'Open',  'TSLA'),\n",
       "            ('Volume',  'TSLA'),\n",
       "            ( 'Close',  'NVDA'),\n",
       "            (  'High',  'NVDA'),\n",
       "            (   'Low',  'NVDA'),\n",
       "            (  'Open',  'NVDA'),\n",
       "            ('Volume',  'NVDA'),\n",
       "            ( 'Close', 'BRK-B'),\n",
       "            (  'High', 'BRK-B'),\n",
       "            (   'Low', 'BRK-B'),\n",
       "            (  'Open', 'BRK-B'),\n",
       "            ('Volume', 'BRK-B'),\n",
       "            ( 'Close',  'META'),\n",
       "            (  'High',  'META'),\n",
       "            (   'Low',  'META'),\n",
       "            (  'Open',  'META'),\n",
       "            ('Volume',  'META'),\n",
       "            ( 'Close',     'V'),\n",
       "            (  'High',     'V'),\n",
       "            (   'Low',     'V'),\n",
       "            (  'Open',     'V'),\n",
       "            ('Volume',     'V'),\n",
       "            ( 'Close',   'JNJ'),\n",
       "            (  'High',   'JNJ'),\n",
       "            (   'Low',   'JNJ'),\n",
       "            (  'Open',   'JNJ'),\n",
       "            ('Volume',   'JNJ')],\n",
       "           names=['Price', 'Ticker'])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "0fEOJwXeIbs4"
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Length mismatch: Expected axis has 51 elements, new values have 6 elements",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mopen\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhigh\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlow\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclose\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvolume\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mticker\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/pandas/core/generic.py:6218\u001b[0m, in \u001b[0;36mNDFrame.__setattr__\u001b[0;34m(self, name, value)\u001b[0m\n\u001b[1;32m   6216\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   6217\u001b[0m     \u001b[38;5;28mobject\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__getattribute__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name)\n\u001b[0;32m-> 6218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mobject\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__setattr__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   6219\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m:\n\u001b[1;32m   6220\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "File \u001b[0;32mproperties.pyx:69\u001b[0m, in \u001b[0;36mpandas._libs.properties.AxisProperty.__set__\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/pandas/core/generic.py:767\u001b[0m, in \u001b[0;36mNDFrame._set_axis\u001b[0;34m(self, axis, labels)\u001b[0m\n\u001b[1;32m    762\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    763\u001b[0m \u001b[38;5;124;03mThis is called from the cython code when we set the `index` attribute\u001b[39;00m\n\u001b[1;32m    764\u001b[0m \u001b[38;5;124;03mdirectly, e.g. `series.index = [1, 2, 3]`.\u001b[39;00m\n\u001b[1;32m    765\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    766\u001b[0m labels \u001b[38;5;241m=\u001b[39m ensure_index(labels)\n\u001b[0;32m--> 767\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_mgr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mset_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    768\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_clear_item_cache()\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/pandas/core/internals/managers.py:227\u001b[0m, in \u001b[0;36mBaseBlockManager.set_axis\u001b[0;34m(self, axis, new_labels)\u001b[0m\n\u001b[1;32m    225\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mset_axis\u001b[39m(\u001b[38;5;28mself\u001b[39m, axis: AxisInt, new_labels: Index) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    226\u001b[0m     \u001b[38;5;66;03m# Caller is responsible for ensuring we have an Index object.\u001b[39;00m\n\u001b[0;32m--> 227\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_set_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnew_labels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    228\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxes[axis] \u001b[38;5;241m=\u001b[39m new_labels\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/pandas/core/internals/base.py:85\u001b[0m, in \u001b[0;36mDataManager._validate_set_axis\u001b[0;34m(self, axis, new_labels)\u001b[0m\n\u001b[1;32m     82\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m new_len \u001b[38;5;241m!=\u001b[39m old_len:\n\u001b[0;32m---> 85\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m     86\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLength mismatch: Expected axis has \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mold_len\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m elements, new \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     87\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalues have \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnew_len\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m elements\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     88\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: Length mismatch: Expected axis has 51 elements, new values have 6 elements"
     ]
    }
   ],
   "source": [
    "df.columns = ['open', 'high', 'low', 'close', 'volume', 'ticker']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 257
    },
    "id": "WCMKma2oIa79",
    "outputId": "cdf1dbb9-1d6b-4b41-a9a7-43265f51ea7d"
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l6vzCNomPf9z"
   },
   "outputs": [],
   "source": [
    "df = df.reset_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tBl3T6WyPs5s"
   },
   "outputs": [],
   "source": [
    "df = df.rename({\"Date\": \"date\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FBx4eAX5IcgS"
   },
   "source": [
    "## Formating Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZT9F4BRzIfV4"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import List, Dict, Tuple\n",
    "import json\n",
    "\n",
    "def calculate_technical_indicators(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Calculate various technical indicators for stock analysis.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame with at least 'close' price column\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with additional technical indicator columns\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Simple Moving Average (SMA)\n",
    "    df['SMA_20'] = df['close'].rolling(window=20).mean()\n",
    "    df['SMA_50'] = df['close'].rolling(window=50).mean()\n",
    "\n",
    "    # Relative Strength Index (RSI)\n",
    "    delta = df['close'].diff()\n",
    "    gain = (delta.where(delta > 0, 0)).rolling(window=14).mean()\n",
    "    loss = (-delta.where(delta < 0, 0)).rolling(window=14).mean()\n",
    "    rs = gain / loss\n",
    "    df['RSI'] = 100 - (100 / (1 + rs))\n",
    "\n",
    "    # Moving Average Convergence Divergence (MACD)\n",
    "    exp1 = df['close'].ewm(span=12, adjust=False).mean()\n",
    "    exp2 = df['close'].ewm(span=26, adjust=False).mean()\n",
    "    df['MACD'] = exp1 - exp2\n",
    "    df['Signal_Line'] = df['MACD'].ewm(span=9, adjust=False).mean()\n",
    "\n",
    "    return df\n",
    "\n",
    "def analyze_technical_signals(row: pd.Series) -> Tuple[str, List[str]]:\n",
    "    \"\"\"\n",
    "    Analyze technical signals and generate trading recommendations.\n",
    "    \n",
    "    Args:\n",
    "        row (pd.Series): Single row of stock data with technical indicators\n",
    "        \n",
    "    Returns:\n",
    "        Tuple[str, List[str]]: Tuple containing recommendation and list of reasons\n",
    "    \"\"\"\n",
    "    signals = []\n",
    "    \n",
    "    # SMA Crossover Analysis\n",
    "    sma_diff = abs(row['SMA_20'] - row['SMA_50'])\n",
    "    if sma_diff < row['close'] * 0.01:\n",
    "        if row['SMA_20'] > row['SMA_50']:\n",
    "            signals.append((\"BUY\", \"SMA crossover indicates upward momentum\"))\n",
    "        else:\n",
    "            signals.append((\"SELL\", \"SMA crossover indicates downward momentum\"))\n",
    "    \n",
    "    # RSI Analysis\n",
    "    if row['RSI'] > 70:\n",
    "        signals.append((\"SELL\", f\"RSI overbought at {row['RSI']:.2f}\"))\n",
    "    elif row['RSI'] < 30:\n",
    "        signals.append((\"BUY\", f\"RSI oversold at {row['RSI']:.2f}\"))\n",
    "    \n",
    "    # MACD Analysis\n",
    "    macd_diff = abs(row['MACD'] - row['Signal_Line'])\n",
    "    if macd_diff < row['close'] * 0.001:\n",
    "        if row['MACD'] > row['Signal_Line']:\n",
    "            signals.append((\"BUY\", \"MACD indicates bullish trend\"))\n",
    "        else:\n",
    "            signals.append((\"SELL\", \"MACD indicates bearish trend\"))\n",
    "    \n",
    "    # Determine final recommendation\n",
    "    if not signals:\n",
    "        return \"HOLD\", [\"No strong technical signals detected\"]\n",
    "    \n",
    "    buy_signals = sum(1 for signal in signals if signal[0] == \"BUY\")\n",
    "    sell_signals = sum(1 for signal in signals if signal[0] == \"SELL\")\n",
    "    \n",
    "    if buy_signals > sell_signals:\n",
    "        recommendation = \"BUY\"\n",
    "    elif sell_signals > buy_signals:\n",
    "        recommendation = \"SELL\"\n",
    "    else:\n",
    "        recommendation = \"HOLD\"\n",
    "    \n",
    "    reasons = [signal[1] for signal in signals]\n",
    "    return recommendation, reasons\n",
    "\n",
    "def format_stock_data_for_llm(stock_data: pd.DataFrame) -> List[Dict[str, str]]:\n",
    "    \"\"\"\n",
    "    Format stock data for language model fine-tuning with instructions.\n",
    "    \n",
    "    Args:\n",
    "        stock_data (pd.DataFrame): DataFrame containing stock price and volume data\n",
    "        \n",
    "    Returns:\n",
    "        List[Dict[str, str]]: Formatted data with instruction, input, and output fields\n",
    "    \"\"\"\n",
    "    required_columns = ['ticker', 'date', 'open', 'high', 'low', 'close', 'volume']\n",
    "    if not all(col in stock_data.columns for col in required_columns):\n",
    "        raise ValueError(f\"Missing required columns: {required_columns}\")\n",
    "\n",
    "    formatted_data = []\n",
    "    stock_data = stock_data.sort_values('date')\n",
    "    stock_data = calculate_technical_indicators(stock_data)\n",
    "\n",
    "    for _, row in stock_data.iterrows():\n",
    "        # Generate the instruction\n",
    "        instruction = \"Analyze the provided stock data and technical indicators to generate a trading recommendation with detailed explanations.\"\n",
    "\n",
    "        # Format input data\n",
    "        input_text = f\"\"\"Stock: {row['ticker']}\n",
    "Date: {row['date']}\n",
    "Open: {row['open']:.2f}\n",
    "High: {row['high']:.2f}\n",
    "Low: {row['low']:.2f}\n",
    "Close: {row['close']:.2f}\n",
    "Volume: {row['volume']:,}\n",
    "\n",
    "Technical Indicators:\n",
    "- SMA (20-day): {row['SMA_20']:.2f}\n",
    "- SMA (50-day): {row['SMA_50']:.2f}\n",
    "- RSI (14-day): {row['RSI']:.2f}\n",
    "- MACD: {row['MACD']:.2f}\n",
    "- MACD Signal Line: {row['Signal_Line']:.2f}\"\"\"\n",
    "\n",
    "        # Generate recommendation\n",
    "        recommendation, reasons = analyze_technical_signals(row)\n",
    "\n",
    "        # Format output text\n",
    "        output_text = f\"\"\"Based on the analysis of {row['ticker']}, the recommendation is to {recommendation}.\n",
    "\n",
    "Technical Analysis Breakdown:\n",
    "{chr(10).join(f\"- {reason}\" for reason in reasons)}\n",
    "\n",
    "Key Metrics Summary:\n",
    "- Closing Price: ${row['close']:.2f}\n",
    "- Trading Volume: {row['volume']:,}\n",
    "- Technical Levels:\n",
    "  * 20-day SMA: ${row['SMA_20']:.2f}\n",
    "  * 50-day SMA: ${row['SMA_50']:.2f}\n",
    "  * RSI: {row['RSI']:.2f}\n",
    "  * MACD: {row['MACD']:.2f}\n",
    "  * Signal Line: {row['Signal_Line']:.2f}\n",
    "\n",
    "Note: This analysis is based on technical indicators only. Consider fundamental factors and consult financial advisors before making investment decisions.\"\"\"\n",
    "\n",
    "        formatted_data.append({\n",
    "            \"instruction\": instruction,\n",
    "            \"input\": input_text,\n",
    "            \"output\": output_text\n",
    "        })\n",
    "\n",
    "    return formatted_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Cwm3ncZUP3M5"
   },
   "outputs": [],
   "source": [
    "df.rename(columns={'Date': 'date'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 226
    },
    "id": "sWRk-eNmP9B6",
    "outputId": "ac236e96-0e4f-4274-d03c-5c804ed23c72"
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MoSAbl4CIhia"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Example usage:\n",
    "# stock_data = pd.read_csv('stock_data.csv')\n",
    "formatted_data = format_stock_data_for_llm(df)\n",
    "\n",
    "# # Save the formatted data to a JSON file\n",
    "# with open('formatted_stock_data_for_gemma.json', 'w') as f:\n",
    "#     json.dump(formatted_data, f, indent=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jTsLVXx9QsD5"
   },
   "outputs": [],
   "source": [
    "# # Save the formatted data to a JSON file\n",
    "with open('formatted_stock_data_for_gemma.json', 'w') as f:\n",
    "    json.dump(formatted_data, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "9nNlqA-KQFKA",
    "outputId": "1b0b0885-5751-4c97-8d36-3912951563fa"
   },
   "outputs": [],
   "source": [
    "formatted_data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Formatted Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "tnTBKT1MQM_Y"
   },
   "outputs": [],
   "source": [
    "# read the file \n",
    "import json\n",
    "\n",
    "dataset = None\n",
    "# Read JSON file\n",
    "with open('formatted_stock_data_for_gemma.json', 'r', encoding='utf-8') as file:\n",
    "    dataset = json.load(file)\n",
    "\n",
    "# Now you can use the data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'instruction': 'Analyze the provided stock data and technical indicators to generate a trading recommendation with detailed explanations.',\n",
       " 'input': 'Stock: AAPL\\nDate: 2023-01-03 00:00:00\\nOpen: 130.28\\nHigh: 130.90\\nLow: 124.17\\nClose: 125.07\\nVolume: 112,117,500\\n\\nTechnical Indicators:\\n- SMA (20-day): nan\\n- SMA (50-day): nan\\n- RSI (14-day): nan\\n- MACD: 0.00\\n- MACD Signal Line: 0.00',\n",
       " 'output': 'Based on the analysis of AAPL, the recommendation is to SELL.\\n\\nTechnical Analysis Breakdown:\\n- MACD indicates bearish trend\\n\\nKey Metrics Summary:\\n- Closing Price: $125.07\\n- Trading Volume: 112,117,500\\n- Technical Levels:\\n  * 20-day SMA: $nan\\n  * 50-day SMA: $nan\\n  * RSI: nan\\n  * MACD: 0.00\\n  * Signal Line: 0.00\\n\\nNote: This analysis is based on technical indicators only. Consider fundamental factors and consult financial advisors before making investment decisions.'}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# /load_dataset('csv', data_files='my_file.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lg05hpvnQMFJ"
   },
   "source": [
    "## Prepare for Fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Channels:\n",
      " - conda-forge\n",
      " - defaults\n",
      "Platform: linux-64\n",
      "Collecting package metadata (repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "# All requested packages already installed.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!conda install -c conda-forge huggingface_hub\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "Ia31lWEqQLqU",
    "outputId": "cb58395b-1a75-4a9a-d146-79139166d219"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (24.2)\n",
      "Collecting unsloth@ git+https://github.com/unslothai/unsloth.git (from unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git)\n",
      "  Cloning https://github.com/unslothai/unsloth.git to /tmp/pip-install-m9bkd69b/unsloth_66d6b25fe2514f4aa81434928b437656\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/unslothai/unsloth.git /tmp/pip-install-m9bkd69b/unsloth_66d6b25fe2514f4aa81434928b437656\n",
      "\n",
      "  Resolved https://github.com/unslothai/unsloth.git to commit d76eda4f66828d66aa6a1b01a0d03323e43810dd\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: unsloth-zoo in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2024.10.4)\n",
      "Requirement already satisfied: packaging in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (24.1)\n",
      "Requirement already satisfied: tyro in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.8.14)\n",
      "Requirement already satisfied: transformers>=4.44.2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (4.46.0)\n",
      "Requirement already satisfied: datasets>=2.16.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (3.0.1)\n",
      "Requirement already satisfied: sentencepiece>=0.2.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.2.0)\n",
      "Requirement already satisfied: tqdm in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (4.66.5)\n",
      "Requirement already satisfied: psutil in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (6.0.0)\n",
      "Requirement already satisfied: wheel>=0.42.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.43.0)\n",
      "Requirement already satisfied: numpy in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (1.26.4)\n",
      "Requirement already satisfied: protobuf<4.0.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (3.20.3)\n",
      "Requirement already satisfied: huggingface-hub in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.26.1)\n",
      "Requirement already satisfied: hf-transfer in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.1.8)\n",
      "Requirement already satisfied: filelock in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (3.15.4)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (17.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.3.8)\n",
      "Requirement already satisfied: pandas in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2.1.4)\n",
      "Requirement already satisfied: requests>=2.32.2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2.32.3)\n",
      "Requirement already satisfied: xxhash in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (3.5.0)\n",
      "Requirement already satisfied: multiprocess in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2024.6.1)\n",
      "Requirement already satisfied: aiohttp in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (3.10.5)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (6.0.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from huggingface-hub->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (4.12.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from transformers>=4.44.2->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2024.9.11)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from transformers>=4.44.2->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.4.5)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from transformers>=4.44.2->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.20.1)\n",
      "Requirement already satisfied: docstring-parser>=0.16 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from tyro->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.16)\n",
      "Requirement already satisfied: rich>=11.1.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from tyro->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (13.8.0)\n",
      "Requirement already satisfied: shtab>=1.5.6 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from tyro->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (1.7.1)\n",
      "Requirement already satisfied: torch in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from unsloth-zoo->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2.2.0)\n",
      "Requirement already satisfied: triton in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from unsloth-zoo->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2.2.0)\n",
      "Requirement already satisfied: accelerate>=0.34.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from unsloth-zoo->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (1.0.1)\n",
      "Requirement already satisfied: trl!=0.9.0,!=0.9.1,!=0.9.2,!=0.9.3,<=0.11.1,>=0.7.9 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from unsloth-zoo->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.8.6)\n",
      "Requirement already satisfied: peft!=0.11.0,>=0.7.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from unsloth-zoo->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.13.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from aiohttp->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from aiohttp->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from aiohttp->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from aiohttp->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from aiohttp->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from aiohttp->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (1.9.11)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from aiohttp->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (4.0.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests>=2.32.2->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests>=2.32.2->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests>=2.32.2->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests>=2.32.2->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2024.8.30)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from rich>=11.1.0->tyro->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from rich>=11.1.0->tyro->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2.18.0)\n",
      "Requirement already satisfied: sympy in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch->unsloth-zoo->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (1.13.2)\n",
      "Requirement already satisfied: networkx in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch->unsloth-zoo->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (3.3)\n",
      "Requirement already satisfied: jinja2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch->unsloth-zoo->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (3.1.4)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch->unsloth-zoo->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch->unsloth-zoo->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch->unsloth-zoo->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch->unsloth-zoo->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch->unsloth-zoo->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch->unsloth-zoo->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch->unsloth-zoo->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch->unsloth-zoo->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch->unsloth-zoo->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch->unsloth-zoo->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2.19.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch->unsloth-zoo->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (12.1.105)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch->unsloth-zoo->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (12.6.68)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from pandas->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from pandas->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from pandas->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2024.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich>=11.1.0->tyro->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (0.1.2)\n",
      "Requirement already satisfied: six>=1.5 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets>=2.16.0->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from jinja2->torch->unsloth-zoo->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (2.1.5)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from sympy->torch->unsloth-zoo->unsloth@ git+https://github.com/unslothai/unsloth.git->unsloth[colab-new]@ git+https://github.com/unslothai/unsloth.git) (1.3.0)\n",
      "Requirement already satisfied: xformers in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (0.0.24)\n",
      "Requirement already satisfied: trl<0.9.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (0.8.6)\n",
      "Requirement already satisfied: peft in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (0.13.2)\n",
      "Requirement already satisfied: accelerate in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (1.0.1)\n",
      "Requirement already satisfied: bitsandbytes in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (0.44.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade pip \n",
    "!pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
    "!pip install --no-deps xformers \"trl<0.9.0\" peft accelerate bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "\n",
    "os.environ[\"HF_TOKEN\"]= \"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\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pip install \"unsloth[cu121-ampere-torch220] @ git+https://github.com/unslothai/unsloth.git\"\n"
     ]
    }
   ],
   "source": [
    "!wget -qO- https://raw.githubusercontent.com/unslothai/unsloth/main/unsloth/_auto_install.py | python -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69,
     "referenced_widgets": [
      "2d14286158e248aa907fa76471c2a0cd",
      "1e855f8b44dc4655bb787d9bb28641b8",
      "d60ea0a87cb641d1875b07d826bcbbec",
      "6e40aec31ca741e69228e776a4a03218",
      "ace05913441041738f610becaf34a736",
      "31371a7420db4ecc864aa8a676d8c886",
      "69289f1f513f4f0e95506ee0093176ee",
      "3142cc4a39584e0a8d4df75c127f52df",
      "39b9ade04c44443fbfedbe9f2c118100",
      "63f39efb08924f108725076201d93e3b",
      "bd6e0933248d4eafa72fbcbd8c1c3eee"
     ]
    },
    "collapsed": true,
    "id": "3NAxEGHzQxs_",
    "outputId": "deaf64a5-4d41-4eb1-d214-0396d0e12ed4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "==((====))==  Unsloth 2024.10.7: Fast Gemma patching. Transformers = 4.46.0.\n",
      "   \\\\   /|    GPU: NVIDIA A10G. Max memory: 22.191 GB. Platform = Linux.\n",
      "O^O/ \\_/ \\    Pytorch: 2.2.0+cu121. CUDA = 8.6. CUDA Toolkit = 12.1.\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.24. FA2 = False]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`config.hidden_act` is ignored, you should use `config.hidden_activation` instead.\n",
      "Gemma's activation function will be set to `gelu_pytorch_tanh`. Please, use\n",
      "`config.hidden_activation` if you want to override this behaviour.\n",
      "See https://github.com/huggingface/transformers/pull/29402 for more details.\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "max_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!\n",
    "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n",
    "# 4bit pre quantized models we support for 4x faster downloading + no OOMs.\n",
    "# More models at https://huggingface.co/unsloth\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "model_name = \"unsloth/gemma-7b-bnb-4bit\", # Choose ANY! eg teknium/OpenHermes- 2.5-Mistral-7B\n",
    "max_seq_length = max_seq_length,\n",
    "dtype = dtype,\n",
    "load_in_4bit = load_in_4bit,\n",
    "# token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 544
    },
    "id": "d7FOZZ8kQ_aM",
    "outputId": "bc222881-7354-4986-8034-762006e6ae99"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2024.10.7 patched 28 layers with 28 QKV layers, 28 O layers and 28 MLP layers.\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 16, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "    \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
    "    lora_alpha = 16,\n",
    "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
    "    bias = \"none\",\n",
    "    # Supports any, but = \"none\" is optimized\n",
    "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
    "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
    "    random_state = 3407,\n",
    "    use_rslora = False,\n",
    "    # We support rank stabilized LoRA\n",
    "    loftq_config = None, # And LoftQ\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "GRQB0Dx7ZG1Q"
   },
   "outputs": [],
   "source": [
    "EOS_TOKEN = tokenizer.eos_token # Must add EOS_TOKEN\n",
    "\n",
    "# def formatting_prompts_func(examples):\n",
    "#     instructions = examples[\"instruction\"]\n",
    "#     inputs= examples[\"input\"]\n",
    "#     outputs= examples[\"output\"]\n",
    "#     texts = []\n",
    "#     for instruction, input, output in zip(instructions, inputs, outputs):\n",
    "#     # Must add EOS_TOKEN, otherwise your generation will go on forever!\n",
    "#     text = alpaca_prompt.format(instruction, input, output) + EOS_TOKEN\n",
    "#     texts.append(text)\n",
    "#     return { \"text\" : texts, }\n",
    "#     pass\n",
    "#     from datasets import load_dataset\n",
    "#     dataset = load_dataset(\"yahma/alpaca-cleaned\", split = \"train\")\n",
    "#     dataset = dataset.map(formatting_prompts_func, batched = True,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Define the prompt template\n",
    "alpaca_prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction: {}\n",
    "\n",
    "### Input: {}\n",
    "\n",
    "### Response: {}\"\"\"\n",
    "\n",
    "def formatting_prompts_func(examples):\n",
    "    \"\"\"\n",
    "    Format the dataset examples into the desired prompt structure.\n",
    "    \n",
    "    Args:\n",
    "        examples: Dictionary containing batched dataset examples\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with formatted input texts\n",
    "    \"\"\"\n",
    "    instructions = examples[\"instruction\"]\n",
    "    inputs = examples[\"input\"]\n",
    "    outputs = examples[\"output\"]\n",
    "    \n",
    "    texts = []\n",
    "    \n",
    "    # Process all examples in the batch\n",
    "    for instruction, input_text, output in zip(instructions, inputs, outputs):\n",
    "        # Format the text according to the template and add EOS token\n",
    "        text = alpaca_prompt.format(instruction, input_text, output) + tokenizer.eos_token\n",
    "        texts.append(text)\n",
    "    \n",
    "    # Return dictionary with the formatted texts\n",
    "    return {\"input\": texts}\n",
    "\n",
    "# Load and process the dataset\n",
    "dataset = load_dataset('json', data_files='formatted_stock_data_for_gemma.json')\n",
    "\n",
    "# Apply the formatting function to the dataset\n",
    "formatted_dataset = dataset.map(\n",
    "    formatting_prompts_func,\n",
    "    batched=True,\n",
    "    remove_columns=dataset[\"train\"].column_names  # Remove original columns\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training examples: 2000\n",
      "Validation examples: 500\n"
     ]
    }
   ],
   "source": [
    "# Split into train and validation sets (80% train, 20% validation)\n",
    "split_dataset = formatted_dataset[\"train\"].train_test_split(\n",
    "    test_size=0.2,\n",
    "    seed=42  # for reproducibility\n",
    ")\n",
    "\n",
    "train_dataset = split_dataset[\"train\"]\n",
    "val_dataset = split_dataset[\"test\"]\n",
    "\n",
    "print(f\"Training examples: {len(train_dataset)}\")\n",
    "print(f\"Validation examples: {len(val_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "nQEdstvaRoi0"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/transformers/training_args.py:1559: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:323: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `SFTTrainer.__init__`. Use `processing_class` instead.\n",
      "  super().__init__(\n",
      "max_steps is given, it will override any value given in num_train_epochs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training samples: 2000\n",
      "Number of validation samples: 500\n",
      "Training batch size: 8\n",
      "Number of training steps per epoch: 250\n"
     ]
    }
   ],
   "source": [
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "from unsloth import is_bfloat16_supported\n",
    "import os\n",
    "\n",
    "# First, split your dataset\n",
    "split_dataset = dataset[\"train\"].train_test_split(\n",
    "    test_size=0.2,\n",
    "    seed=42,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "train_dataset = split_dataset[\"train\"]\n",
    "eval_dataset = split_dataset[\"test\"]\n",
    "\n",
    "# Calculate training hyperparameters\n",
    "num_train_samples = len(train_dataset)\n",
    "max_steps = 60  # Your specified max steps\n",
    "num_epochs = 3  # Adjust this based on your needs\n",
    "batch_size = 2  # Your specified batch size\n",
    "gradient_accumulation_steps = 4  # Your specified gradient accumulation\n",
    "\n",
    "# Training arguments with evaluation\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"outputs\",\n",
    "    \n",
    "    # Training parameters\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "    max_steps=max_steps,\n",
    "    num_train_epochs=num_epochs,\n",
    "    \n",
    "    # Learning rate and optimization\n",
    "    learning_rate=2e-4,\n",
    "    weight_decay=0.01,\n",
    "    lr_scheduler_type=\"linear\",\n",
    "    warmup_steps=5,\n",
    "    optim=\"adamw_8bit\",\n",
    "    \n",
    "    # Mixed precision training\n",
    "    fp16=not is_bfloat16_supported(),\n",
    "    bf16=is_bfloat16_supported(),\n",
    "    \n",
    "    # Evaluation and logging\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=10,  # Evaluate every 10 steps\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=1,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=20,  # Save model every 20 steps\n",
    "    \n",
    "    # Load best model at the end of training\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"loss\",\n",
    "    greater_is_better=False,\n",
    "    \n",
    "    # Report to tracking system (optional)\n",
    "    report_to=[\"tensorboard\"],\n",
    "    \n",
    "    # Reproducibility\n",
    "    seed=3407,\n",
    "    \n",
    "    # System utilization\n",
    "    dataloader_num_workers=2,\n",
    "    \n",
    "    # Save total limit to prevent disk overflow\n",
    "    save_total_limit=3,\n",
    ")\n",
    "\n",
    "# Initialize the trainer\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    \n",
    "    # Datasets\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,  # Added evaluation dataset\n",
    "    \n",
    "    # Dataset processing\n",
    "    dataset_text_field=\"input\",\n",
    "    max_seq_length=max_seq_length,\n",
    "    dataset_num_proc=2,\n",
    "    packing=False,  # Disabled packing for more stable training\n",
    "    \n",
    "    # Training arguments\n",
    "    args=training_args,\n",
    ")\n",
    "\n",
    "# Optional: Add callbacks for better monitoring\n",
    "from transformers import EarlyStoppingCallback\n",
    "early_stopping = EarlyStoppingCallback(\n",
    "    early_stopping_patience=3,\n",
    "    early_stopping_threshold=0.01\n",
    ")\n",
    "trainer.add_callback(early_stopping)\n",
    "\n",
    "# Optional: Print training information\n",
    "print(f\"Number of training samples: {len(train_dataset)}\")\n",
    "print(f\"Number of validation samples: {len(eval_dataset)}\")\n",
    "print(f\"Training batch size: {batch_size * gradient_accumulation_steps}\")\n",
    "print(f\"Number of training steps per epoch: {len(train_dataset) // (batch_size * gradient_accumulation_steps)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69,
     "referenced_widgets": [
      "7cfae8c04b1a4fcaa7ac44adf2a85adf",
      "d60f2fb4a7494e96af88c1f697846086",
      "5c595d50a3b14cfb89e8043d5b4c6914",
      "cafeda98e01043e0b92db4ce7e059120",
      "e4f8701b2a1143ed8c573fa56fb7ef1d",
      "1e5e0c3df4524f46a68ce9779b4b6b3a",
      "1e3ece336fa44d769a78c07d30e2e9bb",
      "ecf49b2fd4e74c9d917d2d3c84dd20e6",
      "a636be8e9399475fa77c8a18ad55a65c",
      "755329363b8247299b262543f4cc2b84",
      "df7719d65ebb429cb12706602207ef62"
     ]
    },
    "id": "_oz3ihbARusw",
    "outputId": "aae3f9f3-1829-4da2-9742-582150b61d08"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU = NVIDIA A10G. Max memory = 22.191 GB.\n",
      "5.83 GB of memory reserved.\n"
     ]
    }
   ],
   "source": [
    "#@title Show current memory stats\n",
    "gpu_stats = torch.cuda.get_device_properties(0)\n",
    "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
    "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
    "print(f\"{start_gpu_memory} GB of memory reserved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 420
    },
    "id": "LpOKO3S_Ruvn",
    "outputId": "3e8d48c9-cde0-4afa-d8a9-208263226ba4"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "   \\\\   /|    Num examples = 2,000 | Num Epochs = 1\n",
      "O^O/ \\_/ \\    Batch size per device = 2 | Gradient Accumulation steps = 4\n",
      "\\        /    Total batch size = 8 | Total steps = 60\n",
      " \"-____-\"     Number of trainable parameters = 50,003,968\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='60' max='60' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [60/60 08:24, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.787400</td>\n",
       "      <td>0.760216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.784900</td>\n",
       "      <td>0.722466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.663100</td>\n",
       "      <td>0.654675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.659500</td>\n",
       "      <td>0.639593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.638200</td>\n",
       "      <td>0.619781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.615900</td>\n",
       "      <td>0.619371</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch# Start the fine-tuning process\n",
    "trainer_stats = trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analysis for AAPL:\n",
      "Based on the technical indicators for Apple Inc., I would recommend buying shares of this company at its current price level as it appears to be in uptrend based off several factors including; high volume , positive macd line which indicates momentum . Additionally ; sma(2o) -sma()  line are above their respective moving averages suggesting possible support for future prices while rsi () value remains within healthy range indicating no overbought or oversold conditions exist yet overall we can see signs pointing towards bullish sentiment due these findings making me believe investing into apple now could potentially yield good returns going forward !\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "\n",
    "# Define the prompt template\n",
    "stock_prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction: {}\n",
    "\n",
    "### Input: {}\n",
    "\n",
    "### Response: {}\"\"\"\n",
    "\n",
    "def prepare_stock_data(ticker: str, \n",
    "                      close_price: float, \n",
    "                      volume: int, \n",
    "                      sma_20: float, \n",
    "                      sma_50: float, \n",
    "                      rsi: float, \n",
    "                      macd: float, \n",
    "                      signal_line: float) -> str:\n",
    "    \"\"\"Format stock data for model input\"\"\"\n",
    "    return f\"\"\"Stock: {ticker}\n",
    "Close: {close_price:.2f}\n",
    "Volume: {volume:,}\n",
    "\n",
    "Technical Indicators:\n",
    "- SMA (20-day): {sma_20:.2f}\n",
    "- SMA (50-day): {sma_50:.2f}\n",
    "- RSI (14-day): {rsi:.2f}\n",
    "- MACD: {macd:.2f}\n",
    "- MACD Signal Line: {signal_line:.2f}\"\"\"\n",
    "\n",
    "def get_stock_prediction(model, \n",
    "                        tokenizer, \n",
    "                        ticker: str,\n",
    "                        close_price: float,\n",
    "                        volume: int,\n",
    "                        sma_20: float,\n",
    "                        sma_50: float,\n",
    "                        rsi: float,\n",
    "                        macd: float,\n",
    "                        signal_line: float,\n",
    "                        max_new_tokens: int = 256) -> str:\n",
    "    \"\"\"\n",
    "    Generate stock analysis prediction using the fine-tuned model.\n",
    "    \n",
    "    Args:\n",
    "        model: The fine-tuned language model\n",
    "        tokenizer: The model's tokenizer\n",
    "        ticker: Stock ticker symbol\n",
    "        close_price: Current closing price\n",
    "        volume: Trading volume\n",
    "        sma_20: 20-day Simple Moving Average\n",
    "        sma_50: 50-day Simple Moving Average\n",
    "        rsi: Relative Strength Index\n",
    "        macd: MACD line value\n",
    "        signal_line: MACD signal line value\n",
    "        max_new_tokens: Maximum number of tokens to generate\n",
    "        \n",
    "    Returns:\n",
    "        str: Generated analysis and recommendation\n",
    "    \"\"\"\n",
    "    # Enable faster inference\n",
    "    FastLanguageModel.for_inference(model)\n",
    "    \n",
    "    # Prepare the input data\n",
    "    stock_data = prepare_stock_data(\n",
    "        ticker=ticker,\n",
    "        close_price=close_price,\n",
    "        volume=volume,\n",
    "        sma_20=sma_20,\n",
    "        sma_50=sma_50,\n",
    "        rsi=rsi,\n",
    "        macd=macd,\n",
    "        signal_line=signal_line\n",
    "    )\n",
    "    \n",
    "    # Format the prompt\n",
    "    formatted_prompt = stock_prompt.format(\n",
    "        \"Analyze the provided stock data and provide a trading recommendation with detailed explanations.\",\n",
    "        stock_data,\n",
    "        \"\"  # Empty output for generation\n",
    "    )\n",
    "    \n",
    "    # Tokenize input\n",
    "    inputs = tokenizer(\n",
    "        [formatted_prompt],\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        max_length=512  # Adjust based on your model's context window\n",
    "    ).to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    # Generate prediction\n",
    "    with torch.inference_mode():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            use_cache=True,\n",
    "            temperature=0.7,  # Adjust for more/less randomness\n",
    "            top_p=0.9,  # Nucleus sampling\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "            repetition_penalty=1.1  # Prevent repetitive text\n",
    "        )\n",
    "    \n",
    "    # Decode and return prediction\n",
    "    prediction = tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]\n",
    "    \n",
    "    # Extract only the generated response (remove the input prompt)\n",
    "    response_start = prediction.find(\"### Response:\") + len(\"### Response:\")\n",
    "    return prediction[response_start:].strip()\n",
    "\n",
    "# Example usage\n",
    "example_data = {\n",
    "        \"ticker\": \"AAPL\",\n",
    "        \"close_price\": 175.50,\n",
    "        \"volume\": 82345678,\n",
    "        \"sma_20\": 173.25,\n",
    "        \"sma_50\": 170.80,\n",
    "        \"rsi\": 65.4,\n",
    "        \"macd\": 0.75,\n",
    "        \"signal_line\": 0.50\n",
    "    }\n",
    "    \n",
    "    \n",
    "    # Load your fine-tuned model and tokenizer\n",
    "    # model = ... # Your loaded model\n",
    "    # tokenizer = ... # Your loaded tokenizer\n",
    "    \n",
    "# Generate prediction\n",
    "prediction = get_stock_prediction(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    **example_data\n",
    ")\n",
    "\n",
    "print(f\"Analysis for {example_data['ticker']}:\")\n",
    "print(prediction)\n",
    "\n",
    "# Batch inference example\n",
    "# def batch_stock_predictions(model, tokenizer, stock_data_list: list) -> list:\n",
    "#     \"\"\"\n",
    "#     Generate predictions for multiple stocks at once.\n",
    "    \n",
    "#     Args:\n",
    "#         model: The fine-tuned language model\n",
    "#         tokenizer: The model's tokenizer\n",
    "#         stock_data_list: List of dictionaries containing stock data\n",
    "        \n",
    "#     Returns:\n",
    "#         list: List of predictions for each stock\n",
    "#     \"\"\"\n",
    "#     predictions = []\n",
    "#     for stock_data in stock_data_list:\n",
    "#         prediction = get_stock_prediction(\n",
    "#             model=model,\n",
    "    #         tokenizer=tokenizer,\n",
    "    #         **stock_data\n",
    "    #     )\n",
    "    #     predictions.append({\n",
    "    #         \"ticker\": stock_data[\"ticker\"],\n",
    "    #         \"prediction\": prediction\n",
    "    #     })\n",
    "    # return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving and Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('stock_model/tokenizer_config.json',\n",
       " 'stock_model/special_tokens_map.json',\n",
       " 'stock_model/tokenizer.model',\n",
       " 'stock_model/added_tokens.json',\n",
       " 'stock_model/tokenizer.json')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save_pretrained(\"stock_model\") # Local saving\n",
    "tokenizer.save_pretrained(\"stock_model\")\n",
    "# model.push_to_hub(\"your_name/lora_model\", token = \"...\") # Online saving\n",
    "# tokenizer.push_to_hub(\"your_name/lora_model\", token = \"...\") # Online saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2024.10.6: Fast Gemma patching. Transformers = 4.46.0.\n",
      "   \\\\   /|    GPU: NVIDIA A10G. Max memory: 22.191 GB. Platform = Linux.\n",
      "O^O/ \\_/ \\    Pytorch: 2.2.0+cu121. CUDA = 8.6. CUDA Toolkit = 12.1.\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.24. FA2 = False]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "could not get source code",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 26\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# Load model\u001b[39;00m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:  \u001b[38;5;66;03m# Changed to True for execution\u001b[39;00m\n\u001b[0;32m---> 26\u001b[0m     model, tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mFastLanguageModel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlora_model\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Your trained model path\u001b[39;49;00m\n\u001b[1;32m     28\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_seq_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_seq_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[43m        \u001b[49m\u001b[43mload_in_4bit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mload_in_4bit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     33\u001b[0m     FastLanguageModel\u001b[38;5;241m.\u001b[39mfor_inference(model)  \u001b[38;5;66;03m# Enable faster inference\u001b[39;00m\n\u001b[1;32m     35\u001b[0m     \u001b[38;5;66;03m# Example stock data\u001b[39;00m\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/unsloth/models/loader.py:332\u001b[0m, in \u001b[0;36mFastLanguageModel.from_pretrained\u001b[0;34m(model_name, max_seq_length, dtype, load_in_4bit, token, device_map, rope_scaling, fix_tokenizer, trust_remote_code, use_gradient_checkpointing, resize_model_vocab, revision, *args, **kwargs)\u001b[0m\n\u001b[1;32m    329\u001b[0m     tokenizer_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    330\u001b[0m \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m--> 332\u001b[0m model, tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mdispatch_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    333\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m        \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    334\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_seq_length\u001b[49m\u001b[43m    \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmax_seq_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    335\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m             \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    336\u001b[0m \u001b[43m    \u001b[49m\u001b[43mload_in_4bit\u001b[49m\u001b[43m      \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mload_in_4bit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    337\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m             \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    338\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m        \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    339\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrope_scaling\u001b[49m\u001b[43m      \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mrope_scaling\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    340\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfix_tokenizer\u001b[49m\u001b[43m     \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mfix_tokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    341\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_patcher\u001b[49m\u001b[43m     \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdispatch_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    342\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtokenizer_name\u001b[49m\u001b[43m    \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtokenizer_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    343\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    344\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m          \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mis_peft\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    345\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    346\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m resize_model_vocab \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    349\u001b[0m     model\u001b[38;5;241m.\u001b[39mresize_token_embeddings(resize_model_vocab)\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/unsloth/models/llama.py:1786\u001b[0m, in \u001b[0;36mFastLlamaModel.from_pretrained\u001b[0;34m(model_name, max_seq_length, dtype, load_in_4bit, token, device_map, rope_scaling, fix_tokenizer, model_patcher, tokenizer_name, trust_remote_code, **kwargs)\u001b[0m\n\u001b[1;32m   1783\u001b[0m Trainer\u001b[38;5;241m.\u001b[39m_inner_training_loop \u001b[38;5;241m=\u001b[39m _fast_inner_training_loop\n\u001b[1;32m   1785\u001b[0m \u001b[38;5;66;03m# Fix gradient accumulation\u001b[39;00m\n\u001b[0;32m-> 1786\u001b[0m \u001b[43mpatch_gradient_accumulation_fix\u001b[49m\u001b[43m(\u001b[49m\u001b[43mTrainer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1788\u001b[0m \u001b[38;5;66;03m# Save tokenizer for inference purposes\u001b[39;00m\n\u001b[1;32m   1789\u001b[0m tokenizer\u001b[38;5;241m.\u001b[39mpadding_side \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mleft\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;66;03m# Force inference\u001b[39;00m\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/unsloth/models/_utils.py:1205\u001b[0m, in \u001b[0;36mpatch_gradient_accumulation_fix\u001b[0;34m(Trainer)\u001b[0m\n\u001b[1;32m   1202\u001b[0m \u001b[38;5;66;03m# Also fix up loss scaling ie negate loss *= self.args.gradient_accumulation_steps\u001b[39;00m\n\u001b[1;32m   1203\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_items_in_batch\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39msignature(Trainer\u001b[38;5;241m.\u001b[39mtraining_step)\u001b[38;5;241m.\u001b[39mparameters: \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m-> 1205\u001b[0m function \u001b[38;5;241m=\u001b[39m \u001b[43minspect\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetsource\u001b[49m\u001b[43m(\u001b[49m\u001b[43mTrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1206\u001b[0m where \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdef\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1207\u001b[0m function \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/inspect.py:1139\u001b[0m, in \u001b[0;36mgetsource\u001b[0;34m(object)\u001b[0m\n\u001b[1;32m   1133\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgetsource\u001b[39m(\u001b[38;5;28mobject\u001b[39m):\n\u001b[1;32m   1134\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return the text of the source code for an object.\u001b[39;00m\n\u001b[1;32m   1135\u001b[0m \n\u001b[1;32m   1136\u001b[0m \u001b[38;5;124;03m    The argument may be a module, class, method, function, traceback, frame,\u001b[39;00m\n\u001b[1;32m   1137\u001b[0m \u001b[38;5;124;03m    or code object.  The source code is returned as a single string.  An\u001b[39;00m\n\u001b[1;32m   1138\u001b[0m \u001b[38;5;124;03m    OSError is raised if the source code cannot be retrieved.\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1139\u001b[0m     lines, lnum \u001b[38;5;241m=\u001b[39m \u001b[43mgetsourcelines\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mobject\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1140\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(lines)\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/inspect.py:1121\u001b[0m, in \u001b[0;36mgetsourcelines\u001b[0;34m(object)\u001b[0m\n\u001b[1;32m   1113\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Return a list of source lines and starting line number for an object.\u001b[39;00m\n\u001b[1;32m   1114\u001b[0m \n\u001b[1;32m   1115\u001b[0m \u001b[38;5;124;03mThe argument may be a module, class, method, function, traceback, frame,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1118\u001b[0m \u001b[38;5;124;03moriginal source file the first line of code was found.  An OSError is\u001b[39;00m\n\u001b[1;32m   1119\u001b[0m \u001b[38;5;124;03mraised if the source code cannot be retrieved.\"\"\"\u001b[39;00m\n\u001b[1;32m   1120\u001b[0m \u001b[38;5;28mobject\u001b[39m \u001b[38;5;241m=\u001b[39m unwrap(\u001b[38;5;28mobject\u001b[39m)\n\u001b[0;32m-> 1121\u001b[0m lines, lnum \u001b[38;5;241m=\u001b[39m \u001b[43mfindsource\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mobject\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1123\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m istraceback(\u001b[38;5;28mobject\u001b[39m):\n\u001b[1;32m   1124\u001b[0m     \u001b[38;5;28mobject\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mobject\u001b[39m\u001b[38;5;241m.\u001b[39mtb_frame\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/inspect.py:958\u001b[0m, in \u001b[0;36mfindsource\u001b[0;34m(object)\u001b[0m\n\u001b[1;32m    956\u001b[0m     lines \u001b[38;5;241m=\u001b[39m linecache\u001b[38;5;241m.\u001b[39mgetlines(file)\n\u001b[1;32m    957\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m lines:\n\u001b[0;32m--> 958\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcould not get source code\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    960\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ismodule(\u001b[38;5;28mobject\u001b[39m):\n\u001b[1;32m    961\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m lines, \u001b[38;5;241m0\u001b[39m\n",
      "\u001b[0;31mOSError\u001b[0m: could not get source code"
     ]
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "\n",
    "def prepare_stock_data(ticker: str, \n",
    "                      close_price: float, \n",
    "                      volume: int, \n",
    "                      sma_20: float, \n",
    "                      sma_50: float, \n",
    "                      rsi: float, \n",
    "                      macd: float, \n",
    "                      signal_line: float) -> str:\n",
    "    \"\"\"Format stock data for model input\"\"\"\n",
    "    return f\"\"\"Stock: {ticker}\n",
    "Close: {close_price:.2f}\n",
    "Volume: {volume:,}\n",
    "\n",
    "Technical Indicators:\n",
    "- SMA (20-day): {sma_20:.2f}\n",
    "- SMA (50-day): {sma_50:.2f}\n",
    "- RSI (14-day): {rsi:.2f}\n",
    "- MACD: {macd:.2f}\n",
    "- MACD Signal Line: {signal_line:.2f}\"\"\"\n",
    "\n",
    "# Load model\n",
    "if True:  # Changed to True for execution\n",
    "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "        model_name=\"lora_model\",  # Your trained model path\n",
    "        max_seq_length=max_seq_length,\n",
    "        dtype=dtype,\n",
    "        load_in_4bit=load_in_4bit,\n",
    "    )\n",
    "    \n",
    "    FastLanguageModel.for_inference(model)  # Enable faster inference\n",
    "    \n",
    "    # Example stock data\n",
    "    example_data = {\n",
    "        \"ticker\": \"AAPL\",\n",
    "        \"close_price\": 175.50,\n",
    "        \"volume\": 82345678,\n",
    "        \"sma_20\": 173.25,\n",
    "        \"sma_50\": 170.80,\n",
    "        \"rsi\": 65.4,\n",
    "        \"macd\": 0.75,\n",
    "        \"signal_line\": 0.50\n",
    "    }\n",
    "    \n",
    "    # Prepare the formatted stock data\n",
    "    stock_data = prepare_stock_data(**example_data)\n",
    "    \n",
    "    # Format the complete prompt\n",
    "    stock_prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction: {}\n",
    "\n",
    "### Input: {}\n",
    "\n",
    "### Response: {}\"\"\"\n",
    "    \n",
    "    formatted_prompt = stock_prompt.format(\n",
    "        \"Analyze the provided stock data and provide a trading recommendation with detailed explanations.\",\n",
    "        stock_data,\n",
    "        \"\"  # Empty output for generation\n",
    "    )\n",
    "    \n",
    "    # Tokenize input\n",
    "    inputs = tokenizer(\n",
    "        [formatted_prompt],\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        max_length=512\n",
    "    ).to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    # Generate prediction\n",
    "    with torch.inference_mode():\n",
    "        outputs = model.generate(\n",
    "            **inputs,  # Use tokenized inputs, not example_data\n",
    "            max_new_tokens=64,\n",
    "            use_cache=True,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "            repetition_penalty=1.1\n",
    "        )\n",
    "    \n",
    "    # Decode prediction\n",
    "    prediction = tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]\n",
    "    \n",
    "    # Extract only the generated response\n",
    "    response_start = prediction.find(\"### Response:\") + len(\"### Response:\")\n",
    "    final_prediction = prediction[response_start:].strip()\n",
    "    \n",
    "    print(f\"Analysis for {example_data['ticker']}:\")\n",
    "    print(final_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # For Vllm\n",
    "# # Merge to 16bit\n",
    "# if False: model.save_pretrained_merged(\"model\", tokenizer, save_method =\n",
    "# \"merged_16bit\",)\n",
    "# if False: model.push_to_hub_merged(\"hf/model\", tokenizer, save_method =\n",
    "# \"merged_16bit\", token = \"\")\n",
    "# # Merge to 4bit\n",
    "# if False: model.save_pretrained_merged(\"model\", tokenizer, save_method =\n",
    "# \"merged_4bit\",)\n",
    "# if False: model.push_to_hub_merged(\"hf/model\", tokenizer, save_method =\n",
    "# \"merged_4bit\", token = \"\")\n",
    "# # Just LoRA adapters\n",
    "# if False: model.save_pretrained_merged(\"model\", tokenizer, save_method = \"lora\",)\n",
    "# if False: model.push_to_hub_merged(\"hf/model\", tokenizer, save_method = \"lora\",\n",
    "# token = \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "make: Entering directory '/teamspace/studios/this_studio/llama.cpp'\n",
      "I ccache not found. Consider installing it for faster compilation.\n",
      "I llama.cpp build info: \n",
      "I UNAME_S:   Linux\n",
      "I UNAME_P:   x86_64\n",
      "I UNAME_M:   x86_64\n",
      "I CFLAGS:    -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE -DGGML_USE_AMX  -std=c11   -fPIC -O3 -g -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -pthread -march=native -mtune=native -fopenmp -Wdouble-promotion \n",
      "I CXXFLAGS:  -std=c++11 -fPIC -O3 -g -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE -DGGML_USE_AMX \n",
      "I NVCCFLAGS: -std=c++11 -O3 -g \n",
      "I LDFLAGS:    \n",
      "I CC:        cc (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0\n",
      "I CXX:       c++ (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0\n",
      "\n",
      "rm -vrf *.dot libllava.a llama-baby-llama llama-batched llama-batched-bench llama-bench llama-cli llama-convert-llama2c-to-ggml llama-embedding llama-eval-callback llama-export-lora llama-gbnf-validator llama-gguf llama-gguf-hash llama-gguf-split llama-gritlm llama-imatrix llama-infill llama-llava-cli llama-minicpmv-cli llama-lookahead llama-lookup llama-lookup-create llama-lookup-merge llama-lookup-stats llama-parallel llama-passkey llama-perplexity llama-q8dot llama-quantize llama-quantize-stats llama-retrieval llama-save-load-state llama-server llama-simple llama-speculative llama-tokenize llama-vdot llama-cvector-generator llama-gen-docs tests/test-c.o tests/test-arg-parser tests/test-autorelease tests/test-backend-ops tests/test-chat-template tests/test-double-float tests/test-grad0 tests/test-grammar-integration tests/test-grammar-parser tests/test-json-schema-to-grammar tests/test-llama-grammar tests/test-log tests/test-model-load-cancel tests/test-opt tests/test-quantize-fns tests/test-quantize-perf tests/test-rope tests/test-sampling tests/test-tokenizer-0 tests/test-tokenizer-1-bpe tests/test-tokenizer-1-spm\n",
      "rm -rvf src/*.o\n",
      "rm -rvf tests/*.o\n",
      "rm -rvf examples/*.o\n",
      "rm -rvf common/*.o\n",
      "rm -rvf *.a\n",
      "rm -rvf *.dll\n",
      "rm -rvf *.so\n",
      "rm -rvf *.dot\n",
      "rm -rvf ggml/*.a\n",
      "rm -rvf ggml/*.dll\n",
      "rm -rvf ggml/*.so\n",
      "rm -vrf ggml/src/*.o\n",
      "rm -rvf ggml/src/llamafile/*.o\n",
      "rm -rvf common/build-info.cpp\n",
      "rm -vrf ggml/src/ggml-metal-embed.metal\n",
      "rm -vrf ggml/src/ggml-cuda/*.o\n",
      "rm -vrf ggml/src/ggml-cuda/template-instances/*.o\n",
      "rm -vrf ggml/src/ggml-amx/*.o\n",
      "rm -rvf libllava.a llama-baby-llama llama-batched llama-batched-bench llama-bench llama-cli llama-convert-llama2c-to-ggml llama-embedding llama-eval-callback llama-export-lora llama-gbnf-validator llama-gguf llama-gguf-hash llama-gguf-split llama-gritlm llama-imatrix llama-infill llama-llava-cli llama-minicpmv-cli llama-lookahead llama-lookup llama-lookup-create llama-lookup-merge llama-lookup-stats llama-parallel llama-passkey llama-perplexity llama-q8dot llama-quantize llama-quantize-stats llama-retrieval llama-save-load-state llama-server llama-simple llama-speculative llama-tokenize llama-vdot llama-cvector-generator llama-gen-docs tests/test-c.o\n",
      "rm -rvf tests/test-arg-parser tests/test-autorelease tests/test-backend-ops tests/test-chat-template tests/test-double-float tests/test-grad0 tests/test-grammar-integration tests/test-grammar-parser tests/test-json-schema-to-grammar tests/test-llama-grammar tests/test-log tests/test-model-load-cancel tests/test-opt tests/test-quantize-fns tests/test-quantize-perf tests/test-rope tests/test-sampling tests/test-tokenizer-0 tests/test-tokenizer-1-bpe tests/test-tokenizer-1-spm\n",
      "rm -f vulkan-shaders-gen ggml/src/ggml-vulkan-shaders.hpp ggml/src/ggml-vulkan-shaders.cpp\n",
      "rm -rvf main quantize quantize-stats perplexity imatrix embedding vdot q8dot convert-llama2c-to-ggml simple batched batched-bench save-load-state server gguf gguf-split eval-callback llama-bench libllava.a llava-cli baby-llama retrieval speculative infill tokenize parallel export-lora lookahead lookup passkey gritlm\n",
      "find examples pocs -type f -name \"*.o\" -delete\n",
      "make: Leaving directory '/teamspace/studios/this_studio/llama.cpp'\n",
      "Unsloth: Merging 4bit and LoRA weights to 16bit...\n",
      "Unsloth: Will use up to 85.03 out of 124.46 RAM for saving.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 23/28 [00:00<00:00, 25.44it/s]We will save to Disk and not RAM now.\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 28/28 [00:05<00:00,  5.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Saving tokenizer... Done.\n",
      "Unsloth: Saving model... This might take 5 minutes for Llama-7b...\n",
      "Done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Converting gemma model. Can use fast conversion = False.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth: Conversion from QLoRA to GGUF information\n",
      "   \\\\   /|    [0] Installing llama.cpp will take 3 minutes.\n",
      "O^O/ \\_/ \\    [1] Converting HF to GGUF 16bits will take 3 minutes.\n",
      "\\        /    [2] Converting GGUF 16bits to ['q4_k_m'] will take 10 minutes each.\n",
      " \"-____-\"     In total, you will have to wait at least 16 minutes.\n",
      "\n",
      "Unsloth: [0] Installing llama.cpp. This will take 3 minutes...\n",
      "Unsloth: [1] Converting model at stock_advisor into bf16 GGUF format.\n",
      "The output location will be /teamspace/studios/this_studio/stock_advisor/unsloth.BF16.gguf\n",
      "This will take 3 minutes...\n",
      "INFO:hf-to-gguf:Loading model: stock_advisor\n",
      "INFO:gguf.gguf_writer:gguf: This GGUF file is for Little Endian only\n",
      "INFO:hf-to-gguf:Exporting model...\n",
      "INFO:hf-to-gguf:gguf: loading model weight map from 'model.safetensors.index.json'\n",
      "INFO:hf-to-gguf:gguf: loading model part 'model-00001-of-00004.safetensors'\n",
      "INFO:hf-to-gguf:token_embd.weight,         torch.bfloat16 --> BF16, shape = {3072, 256000}\n",
      "INFO:hf-to-gguf:blk.0.attn_norm.weight,    torch.bfloat16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.0.ffn_down.weight,     torch.bfloat16 --> BF16, shape = {24576, 3072}\n",
      "INFO:hf-to-gguf:blk.0.ffn_gate.weight,     torch.bfloat16 --> BF16, shape = {3072, 24576}\n",
      "INFO:hf-to-gguf:blk.0.ffn_up.weight,       torch.bfloat16 --> BF16, shape = {3072, 24576}\n",
      "INFO:hf-to-gguf:blk.0.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.0.attn_k.weight,       torch.bfloat16 --> BF16, shape = {3072, 4096}\n",
      "INFO:hf-to-gguf:blk.0.attn_output.weight,  torch.bfloat16 --> BF16, shape = {4096, 3072}\n",
      "INFO:hf-to-gguf:blk.0.attn_q.weight,       torch.bfloat16 --> BF16, shape = {3072, 4096}\n",
      "INFO:hf-to-gguf:blk.0.attn_v.weight,       torch.bfloat16 --> BF16, shape = {3072, 4096}\n",
      "INFO:hf-to-gguf:blk.1.attn_norm.weight,    torch.bfloat16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.1.ffn_down.weight,     torch.bfloat16 --> BF16, shape = {24576, 3072}\n",
      "INFO:hf-to-gguf:blk.1.ffn_gate.weight,     torch.bfloat16 --> BF16, shape = {3072, 24576}\n",
      "INFO:hf-to-gguf:blk.1.ffn_up.weight,       torch.bfloat16 --> BF16, shape = {3072, 24576}\n",
      "INFO:hf-to-gguf:blk.1.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.1.attn_k.weight,       torch.bfloat16 --> BF16, shape = {3072, 4096}\n",
      "INFO:hf-to-gguf:blk.1.attn_output.weight,  torch.bfloat16 --> BF16, shape = {4096, 3072}\n",
      "INFO:hf-to-gguf:blk.1.attn_q.weight,       torch.bfloat16 --> BF16, shape = {3072, 4096}\n",
      "INFO:hf-to-gguf:blk.1.attn_v.weight,       torch.bfloat16 --> BF16, shape = {3072, 4096}\n",
      "INFO:hf-to-gguf:blk.2.attn_norm.weight,    torch.bfloat16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.2.ffn_down.weight,     torch.bfloat16 --> BF16, shape = {24576, 3072}\n",
      "INFO:hf-to-gguf:blk.2.ffn_gate.weight,     torch.bfloat16 --> BF16, shape = {3072, 24576}\n",
      "INFO:hf-to-gguf:blk.2.ffn_up.weight,       torch.bfloat16 --> BF16, shape = {3072, 24576}\n",
      "INFO:hf-to-gguf:blk.2.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.2.attn_k.weight,       torch.bfloat16 --> BF16, shape = {3072, 4096}\n",
      "INFO:hf-to-gguf:blk.2.attn_output.weight,  torch.bfloat16 --> BF16, shape = {4096, 3072}\n",
      "INFO:hf-to-gguf:blk.2.attn_q.weight,       torch.bfloat16 --> BF16, shape = {3072, 4096}\n",
      "INFO:hf-to-gguf:blk.2.attn_v.weight,       torch.bfloat16 --> BF16, shape = {3072, 4096}\n",
      "INFO:hf-to-gguf:blk.3.attn_norm.weight,    torch.bfloat16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.3.ffn_down.weight,     torch.bfloat16 --> BF16, shape = {24576, 3072}\n",
      "INFO:hf-to-gguf:blk.3.ffn_gate.weight,     torch.bfloat16 --> BF16, shape = {3072, 24576}\n",
      "INFO:hf-to-gguf:blk.3.ffn_up.weight,       torch.bfloat16 --> BF16, shape = {3072, 24576}\n",
      "INFO:hf-to-gguf:blk.3.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.3.attn_k.weight,       torch.bfloat16 --> BF16, shape = {3072, 4096}\n",
      "INFO:hf-to-gguf:blk.3.attn_output.weight,  torch.bfloat16 --> BF16, shape = {4096, 3072}\n",
      "INFO:hf-to-gguf:blk.3.attn_q.weight,       torch.bfloat16 --> BF16, shape = {3072, 4096}\n",
      "INFO:hf-to-gguf:blk.3.attn_v.weight,       torch.bfloat16 --> BF16, shape = {3072, 4096}\n",
      "INFO:hf-to-gguf:blk.4.attn_norm.weight,    torch.bfloat16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.4.ffn_down.weight,     torch.bfloat16 --> BF16, shape = {24576, 3072}\n",
      "INFO:hf-to-gguf:blk.4.ffn_gate.weight,     torch.bfloat16 --> BF16, shape = {3072, 24576}\n",
      "INFO:hf-to-gguf:blk.4.ffn_up.weight,       torch.bfloat16 --> BF16, shape = {3072, 24576}\n",
      "INFO:hf-to-gguf:blk.4.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.4.attn_k.weight,       torch.bfloat16 --> BF16, shape = {3072, 4096}\n",
      "INFO:hf-to-gguf:blk.4.attn_output.weight,  torch.bfloat16 --> BF16, shape = {4096, 3072}\n",
      "INFO:hf-to-gguf:blk.4.attn_q.weight,       torch.bfloat16 --> BF16, shape = {3072, 4096}\n",
      "INFO:hf-to-gguf:blk.4.attn_v.weight,       torch.bfloat16 --> BF16, shape = {3072, 4096}\n",
      "INFO:hf-to-gguf:blk.5.attn_norm.weight,    torch.bfloat16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.5.ffn_down.weight,     torch.bfloat16 --> BF16, shape = {24576, 3072}\n",
      "INFO:hf-to-gguf:blk.5.ffn_gate.weight,     torch.bfloat16 --> BF16, shape = {3072, 24576}\n",
      "INFO:hf-to-gguf:blk.5.ffn_up.weight,       torch.bfloat16 --> BF16, shape = {3072, 24576}\n",
      "INFO:hf-to-gguf:blk.5.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.5.attn_k.weight,       torch.bfloat16 --> BF16, shape = {3072, 4096}\n",
      "INFO:hf-to-gguf:blk.5.attn_output.weight,  torch.bfloat16 --> BF16, shape = {4096, 3072}\n",
      "INFO:hf-to-gguf:blk.5.attn_q.weight,       torch.bfloat16 --> BF16, shape = {3072, 4096}\n",
      "INFO:hf-to-gguf:blk.5.attn_v.weight,       torch.bfloat16 --> BF16, shape = {3072, 4096}\n",
      "INFO:hf-to-gguf:blk.6.attn_k.weight,       torch.bfloat16 --> BF16, shape = {3072, 4096}\n",
      "INFO:hf-to-gguf:blk.6.attn_output.weight,  torch.bfloat16 --> BF16, shape = {4096, 3072}\n",
      "INFO:hf-to-gguf:blk.6.attn_q.weight,       torch.bfloat16 --> BF16, shape = {3072, 4096}\n",
      "INFO:hf-to-gguf:blk.6.attn_v.weight,       torch.bfloat16 --> BF16, shape = {3072, 4096}\n",
      "INFO:hf-to-gguf:gguf: loading model part 'model-00002-of-00004.safetensors'\n",
      "INFO:hf-to-gguf:blk.10.attn_norm.weight,   torch.bfloat16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.10.ffn_down.weight,    torch.bfloat16 --> BF16, shape = {24576, 3072}\n",
      "INFO:hf-to-gguf:blk.10.ffn_gate.weight,    torch.bfloat16 --> BF16, shape = {3072, 24576}\n",
      "INFO:hf-to-gguf:blk.10.ffn_up.weight,      torch.bfloat16 --> BF16, shape = {3072, 24576}\n",
      "INFO:hf-to-gguf:blk.10.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.10.attn_k.weight,      torch.bfloat16 --> BF16, shape = {3072, 4096}\n",
      "INFO:hf-to-gguf:blk.10.attn_output.weight, torch.bfloat16 --> BF16, shape = {4096, 3072}\n",
      "INFO:hf-to-gguf:blk.10.attn_q.weight,      torch.bfloat16 --> BF16, shape = {3072, 4096}\n",
      "INFO:hf-to-gguf:blk.10.attn_v.weight,      torch.bfloat16 --> BF16, shape = {3072, 4096}\n",
      "INFO:hf-to-gguf:blk.11.attn_norm.weight,   torch.bfloat16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.11.ffn_down.weight,    torch.bfloat16 --> BF16, shape = {24576, 3072}\n",
      "INFO:hf-to-gguf:blk.11.ffn_gate.weight,    torch.bfloat16 --> BF16, shape = {3072, 24576}\n",
      "INFO:hf-to-gguf:blk.11.ffn_up.weight,      torch.bfloat16 --> BF16, shape = {3072, 24576}\n",
      "INFO:hf-to-gguf:blk.11.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.11.attn_k.weight,      torch.bfloat16 --> BF16, shape = {3072, 4096}\n",
      "INFO:hf-to-gguf:blk.11.attn_output.weight, torch.bfloat16 --> BF16, shape = {4096, 3072}\n",
      "INFO:hf-to-gguf:blk.11.attn_q.weight,      torch.bfloat16 --> BF16, shape = {3072, 4096}\n",
      "INFO:hf-to-gguf:blk.11.attn_v.weight,      torch.bfloat16 --> BF16, shape = {3072, 4096}\n",
      "INFO:hf-to-gguf:blk.12.attn_norm.weight,   torch.bfloat16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.12.ffn_down.weight,    torch.bfloat16 --> BF16, shape = {24576, 3072}\n",
      "INFO:hf-to-gguf:blk.12.ffn_gate.weight,    torch.bfloat16 --> BF16, shape = {3072, 24576}\n",
      "INFO:hf-to-gguf:blk.12.ffn_up.weight,      torch.bfloat16 --> BF16, shape = {3072, 24576}\n",
      "INFO:hf-to-gguf:blk.12.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.12.attn_k.weight,      torch.bfloat16 --> BF16, shape = {3072, 4096}\n",
      "INFO:hf-to-gguf:blk.12.attn_output.weight, torch.bfloat16 --> BF16, shape = {4096, 3072}\n",
      "INFO:hf-to-gguf:blk.12.attn_q.weight,      torch.bfloat16 --> BF16, shape = {3072, 4096}\n",
      "INFO:hf-to-gguf:blk.12.attn_v.weight,      torch.bfloat16 --> BF16, shape = {3072, 4096}\n",
      "INFO:hf-to-gguf:blk.13.attn_norm.weight,   torch.bfloat16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.13.ffn_down.weight,    torch.bfloat16 --> BF16, shape = {24576, 3072}\n",
      "INFO:hf-to-gguf:blk.13.ffn_gate.weight,    torch.bfloat16 --> BF16, shape = {3072, 24576}\n",
      "INFO:hf-to-gguf:blk.13.ffn_up.weight,      torch.bfloat16 --> BF16, shape = {3072, 24576}\n",
      "INFO:hf-to-gguf:blk.13.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.13.attn_k.weight,      torch.bfloat16 --> BF16, shape = {3072, 4096}\n",
      "INFO:hf-to-gguf:blk.13.attn_output.weight, torch.bfloat16 --> BF16, shape = {4096, 3072}\n",
      "INFO:hf-to-gguf:blk.13.attn_q.weight,      torch.bfloat16 --> BF16, shape = {3072, 4096}\n",
      "INFO:hf-to-gguf:blk.13.attn_v.weight,      torch.bfloat16 --> BF16, shape = {3072, 4096}\n",
      "INFO:hf-to-gguf:blk.14.attn_norm.weight,   torch.bfloat16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.14.ffn_down.weight,    torch.bfloat16 --> BF16, shape = {24576, 3072}\n",
      "INFO:hf-to-gguf:blk.14.ffn_gate.weight,    torch.bfloat16 --> BF16, shape = {3072, 24576}\n",
      "INFO:hf-to-gguf:blk.14.ffn_up.weight,      torch.bfloat16 --> BF16, shape = {3072, 24576}\n",
      "INFO:hf-to-gguf:blk.14.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.14.attn_k.weight,      torch.bfloat16 --> BF16, shape = {3072, 4096}\n",
      "INFO:hf-to-gguf:blk.14.attn_output.weight, torch.bfloat16 --> BF16, shape = {4096, 3072}\n",
      "INFO:hf-to-gguf:blk.14.attn_q.weight,      torch.bfloat16 --> BF16, shape = {3072, 4096}\n",
      "INFO:hf-to-gguf:blk.14.attn_v.weight,      torch.bfloat16 --> BF16, shape = {3072, 4096}\n",
      "INFO:hf-to-gguf:blk.15.attn_k.weight,      torch.bfloat16 --> BF16, shape = {3072, 4096}\n",
      "INFO:hf-to-gguf:blk.15.attn_output.weight, torch.bfloat16 --> BF16, shape = {4096, 3072}\n",
      "INFO:hf-to-gguf:blk.15.attn_q.weight,      torch.bfloat16 --> BF16, shape = {3072, 4096}\n",
      "INFO:hf-to-gguf:blk.15.attn_v.weight,      torch.bfloat16 --> BF16, shape = {3072, 4096}\n",
      "INFO:hf-to-gguf:blk.6.attn_norm.weight,    torch.bfloat16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.6.ffn_down.weight,     torch.bfloat16 --> BF16, shape = {24576, 3072}\n",
      "INFO:hf-to-gguf:blk.6.ffn_gate.weight,     torch.bfloat16 --> BF16, shape = {3072, 24576}\n",
      "INFO:hf-to-gguf:blk.6.ffn_up.weight,       torch.bfloat16 --> BF16, shape = {3072, 24576}\n",
      "INFO:hf-to-gguf:blk.6.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.7.attn_norm.weight,    torch.bfloat16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.7.ffn_down.weight,     torch.bfloat16 --> BF16, shape = {24576, 3072}\n",
      "INFO:hf-to-gguf:blk.7.ffn_gate.weight,     torch.bfloat16 --> BF16, shape = {3072, 24576}\n",
      "INFO:hf-to-gguf:blk.7.ffn_up.weight,       torch.bfloat16 --> BF16, shape = {3072, 24576}\n",
      "INFO:hf-to-gguf:blk.7.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.7.attn_k.weight,       torch.bfloat16 --> BF16, shape = {3072, 4096}\n",
      "INFO:hf-to-gguf:blk.7.attn_output.weight,  torch.bfloat16 --> BF16, shape = {4096, 3072}\n",
      "INFO:hf-to-gguf:blk.7.attn_q.weight,       torch.bfloat16 --> BF16, shape = {3072, 4096}\n",
      "INFO:hf-to-gguf:blk.7.attn_v.weight,       torch.bfloat16 --> BF16, shape = {3072, 4096}\n",
      "INFO:hf-to-gguf:blk.8.attn_norm.weight,    torch.bfloat16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.8.ffn_down.weight,     torch.bfloat16 --> BF16, shape = {24576, 3072}\n",
      "INFO:hf-to-gguf:blk.8.ffn_gate.weight,     torch.bfloat16 --> BF16, shape = {3072, 24576}\n",
      "INFO:hf-to-gguf:blk.8.ffn_up.weight,       torch.bfloat16 --> BF16, shape = {3072, 24576}\n",
      "INFO:hf-to-gguf:blk.8.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.8.attn_k.weight,       torch.bfloat16 --> BF16, shape = {3072, 4096}\n",
      "INFO:hf-to-gguf:blk.8.attn_output.weight,  torch.bfloat16 --> BF16, shape = {4096, 3072}\n",
      "INFO:hf-to-gguf:blk.8.attn_q.weight,       torch.bfloat16 --> BF16, shape = {3072, 4096}\n",
      "INFO:hf-to-gguf:blk.8.attn_v.weight,       torch.bfloat16 --> BF16, shape = {3072, 4096}\n",
      "INFO:hf-to-gguf:blk.9.attn_norm.weight,    torch.bfloat16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.9.ffn_down.weight,     torch.bfloat16 --> BF16, shape = {24576, 3072}\n",
      "INFO:hf-to-gguf:blk.9.ffn_gate.weight,     torch.bfloat16 --> BF16, shape = {3072, 24576}\n",
      "INFO:hf-to-gguf:blk.9.ffn_up.weight,       torch.bfloat16 --> BF16, shape = {3072, 24576}\n",
      "INFO:hf-to-gguf:blk.9.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.9.attn_k.weight,       torch.bfloat16 --> BF16, shape = {3072, 4096}\n",
      "INFO:hf-to-gguf:blk.9.attn_output.weight,  torch.bfloat16 --> BF16, shape = {4096, 3072}\n",
      "INFO:hf-to-gguf:blk.9.attn_q.weight,       torch.bfloat16 --> BF16, shape = {3072, 4096}\n",
      "INFO:hf-to-gguf:blk.9.attn_v.weight,       torch.bfloat16 --> BF16, shape = {3072, 4096}\n",
      "INFO:hf-to-gguf:gguf: loading model part 'model-00003-of-00004.safetensors'\n",
      "INFO:hf-to-gguf:blk.15.attn_norm.weight,   torch.bfloat16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.15.ffn_down.weight,    torch.bfloat16 --> BF16, shape = {24576, 3072}\n",
      "INFO:hf-to-gguf:blk.15.ffn_gate.weight,    torch.bfloat16 --> BF16, shape = {3072, 24576}\n",
      "INFO:hf-to-gguf:blk.15.ffn_up.weight,      torch.bfloat16 --> BF16, shape = {3072, 24576}\n",
      "INFO:hf-to-gguf:blk.15.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.16.attn_norm.weight,   torch.bfloat16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.16.ffn_down.weight,    torch.bfloat16 --> BF16, shape = {24576, 3072}\n",
      "INFO:hf-to-gguf:blk.16.ffn_gate.weight,    torch.bfloat16 --> BF16, shape = {3072, 24576}\n",
      "INFO:hf-to-gguf:blk.16.ffn_up.weight,      torch.bfloat16 --> BF16, shape = {3072, 24576}\n",
      "INFO:hf-to-gguf:blk.16.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.16.attn_k.weight,      torch.bfloat16 --> BF16, shape = {3072, 4096}\n",
      "INFO:hf-to-gguf:blk.16.attn_output.weight, torch.bfloat16 --> BF16, shape = {4096, 3072}\n",
      "INFO:hf-to-gguf:blk.16.attn_q.weight,      torch.bfloat16 --> BF16, shape = {3072, 4096}\n",
      "INFO:hf-to-gguf:blk.16.attn_v.weight,      torch.bfloat16 --> BF16, shape = {3072, 4096}\n",
      "INFO:hf-to-gguf:blk.17.attn_norm.weight,   torch.bfloat16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.17.ffn_down.weight,    torch.bfloat16 --> BF16, shape = {24576, 3072}\n",
      "INFO:hf-to-gguf:blk.17.ffn_gate.weight,    torch.bfloat16 --> BF16, shape = {3072, 24576}\n",
      "INFO:hf-to-gguf:blk.17.ffn_up.weight,      torch.bfloat16 --> BF16, shape = {3072, 24576}\n",
      "INFO:hf-to-gguf:blk.17.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.17.attn_k.weight,      torch.bfloat16 --> BF16, shape = {3072, 4096}\n",
      "INFO:hf-to-gguf:blk.17.attn_output.weight, torch.bfloat16 --> BF16, shape = {4096, 3072}\n",
      "INFO:hf-to-gguf:blk.17.attn_q.weight,      torch.bfloat16 --> BF16, shape = {3072, 4096}\n",
      "INFO:hf-to-gguf:blk.17.attn_v.weight,      torch.bfloat16 --> BF16, shape = {3072, 4096}\n",
      "INFO:hf-to-gguf:blk.18.attn_norm.weight,   torch.bfloat16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.18.ffn_down.weight,    torch.bfloat16 --> BF16, shape = {24576, 3072}\n",
      "INFO:hf-to-gguf:blk.18.ffn_gate.weight,    torch.bfloat16 --> BF16, shape = {3072, 24576}\n",
      "INFO:hf-to-gguf:blk.18.ffn_up.weight,      torch.bfloat16 --> BF16, shape = {3072, 24576}\n",
      "INFO:hf-to-gguf:blk.18.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.18.attn_k.weight,      torch.bfloat16 --> BF16, shape = {3072, 4096}\n",
      "INFO:hf-to-gguf:blk.18.attn_output.weight, torch.bfloat16 --> BF16, shape = {4096, 3072}\n",
      "INFO:hf-to-gguf:blk.18.attn_q.weight,      torch.bfloat16 --> BF16, shape = {3072, 4096}\n",
      "INFO:hf-to-gguf:blk.18.attn_v.weight,      torch.bfloat16 --> BF16, shape = {3072, 4096}\n",
      "INFO:hf-to-gguf:blk.19.attn_norm.weight,   torch.bfloat16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.19.ffn_down.weight,    torch.bfloat16 --> BF16, shape = {24576, 3072}\n",
      "INFO:hf-to-gguf:blk.19.ffn_gate.weight,    torch.bfloat16 --> BF16, shape = {3072, 24576}\n",
      "INFO:hf-to-gguf:blk.19.ffn_up.weight,      torch.bfloat16 --> BF16, shape = {3072, 24576}\n",
      "INFO:hf-to-gguf:blk.19.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.19.attn_k.weight,      torch.bfloat16 --> BF16, shape = {3072, 4096}\n",
      "INFO:hf-to-gguf:blk.19.attn_output.weight, torch.bfloat16 --> BF16, shape = {4096, 3072}\n",
      "INFO:hf-to-gguf:blk.19.attn_q.weight,      torch.bfloat16 --> BF16, shape = {3072, 4096}\n",
      "INFO:hf-to-gguf:blk.19.attn_v.weight,      torch.bfloat16 --> BF16, shape = {3072, 4096}\n",
      "INFO:hf-to-gguf:blk.20.attn_norm.weight,   torch.bfloat16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.20.ffn_down.weight,    torch.bfloat16 --> BF16, shape = {24576, 3072}\n",
      "INFO:hf-to-gguf:blk.20.ffn_gate.weight,    torch.bfloat16 --> BF16, shape = {3072, 24576}\n",
      "INFO:hf-to-gguf:blk.20.ffn_up.weight,      torch.bfloat16 --> BF16, shape = {3072, 24576}\n",
      "INFO:hf-to-gguf:blk.20.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.20.attn_k.weight,      torch.bfloat16 --> BF16, shape = {3072, 4096}\n",
      "INFO:hf-to-gguf:blk.20.attn_output.weight, torch.bfloat16 --> BF16, shape = {4096, 3072}\n",
      "INFO:hf-to-gguf:blk.20.attn_q.weight,      torch.bfloat16 --> BF16, shape = {3072, 4096}\n",
      "INFO:hf-to-gguf:blk.20.attn_v.weight,      torch.bfloat16 --> BF16, shape = {3072, 4096}\n",
      "INFO:hf-to-gguf:blk.21.attn_norm.weight,   torch.bfloat16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.21.ffn_down.weight,    torch.bfloat16 --> BF16, shape = {24576, 3072}\n",
      "INFO:hf-to-gguf:blk.21.ffn_gate.weight,    torch.bfloat16 --> BF16, shape = {3072, 24576}\n",
      "INFO:hf-to-gguf:blk.21.ffn_up.weight,      torch.bfloat16 --> BF16, shape = {3072, 24576}\n",
      "INFO:hf-to-gguf:blk.21.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.21.attn_k.weight,      torch.bfloat16 --> BF16, shape = {3072, 4096}\n",
      "INFO:hf-to-gguf:blk.21.attn_output.weight, torch.bfloat16 --> BF16, shape = {4096, 3072}\n",
      "INFO:hf-to-gguf:blk.21.attn_q.weight,      torch.bfloat16 --> BF16, shape = {3072, 4096}\n",
      "INFO:hf-to-gguf:blk.21.attn_v.weight,      torch.bfloat16 --> BF16, shape = {3072, 4096}\n",
      "INFO:hf-to-gguf:blk.22.attn_norm.weight,   torch.bfloat16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.22.ffn_down.weight,    torch.bfloat16 --> BF16, shape = {24576, 3072}\n",
      "INFO:hf-to-gguf:blk.22.ffn_gate.weight,    torch.bfloat16 --> BF16, shape = {3072, 24576}\n",
      "INFO:hf-to-gguf:blk.22.ffn_up.weight,      torch.bfloat16 --> BF16, shape = {3072, 24576}\n",
      "INFO:hf-to-gguf:blk.22.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.22.attn_k.weight,      torch.bfloat16 --> BF16, shape = {3072, 4096}\n",
      "INFO:hf-to-gguf:blk.22.attn_output.weight, torch.bfloat16 --> BF16, shape = {4096, 3072}\n",
      "INFO:hf-to-gguf:blk.22.attn_q.weight,      torch.bfloat16 --> BF16, shape = {3072, 4096}\n",
      "INFO:hf-to-gguf:blk.22.attn_v.weight,      torch.bfloat16 --> BF16, shape = {3072, 4096}\n",
      "INFO:hf-to-gguf:blk.23.attn_norm.weight,   torch.bfloat16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.23.ffn_down.weight,    torch.bfloat16 --> BF16, shape = {24576, 3072}\n",
      "INFO:hf-to-gguf:blk.23.ffn_gate.weight,    torch.bfloat16 --> BF16, shape = {3072, 24576}\n",
      "INFO:hf-to-gguf:blk.23.ffn_up.weight,      torch.bfloat16 --> BF16, shape = {3072, 24576}\n",
      "INFO:hf-to-gguf:blk.23.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.23.attn_k.weight,      torch.bfloat16 --> BF16, shape = {3072, 4096}\n",
      "INFO:hf-to-gguf:blk.23.attn_output.weight, torch.bfloat16 --> BF16, shape = {4096, 3072}\n",
      "INFO:hf-to-gguf:blk.23.attn_q.weight,      torch.bfloat16 --> BF16, shape = {3072, 4096}\n",
      "INFO:hf-to-gguf:blk.23.attn_v.weight,      torch.bfloat16 --> BF16, shape = {3072, 4096}\n",
      "INFO:hf-to-gguf:blk.24.attn_k.weight,      torch.bfloat16 --> BF16, shape = {3072, 4096}\n",
      "INFO:hf-to-gguf:blk.24.attn_output.weight, torch.bfloat16 --> BF16, shape = {4096, 3072}\n",
      "INFO:hf-to-gguf:blk.24.attn_q.weight,      torch.bfloat16 --> BF16, shape = {3072, 4096}\n",
      "INFO:hf-to-gguf:blk.24.attn_v.weight,      torch.bfloat16 --> BF16, shape = {3072, 4096}\n",
      "INFO:hf-to-gguf:gguf: loading model part 'model-00004-of-00004.safetensors'\n",
      "INFO:hf-to-gguf:blk.24.attn_norm.weight,   torch.bfloat16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.24.ffn_down.weight,    torch.bfloat16 --> BF16, shape = {24576, 3072}\n",
      "INFO:hf-to-gguf:blk.24.ffn_gate.weight,    torch.bfloat16 --> BF16, shape = {3072, 24576}\n",
      "INFO:hf-to-gguf:blk.24.ffn_up.weight,      torch.bfloat16 --> BF16, shape = {3072, 24576}\n",
      "INFO:hf-to-gguf:blk.24.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.25.attn_norm.weight,   torch.bfloat16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.25.ffn_down.weight,    torch.bfloat16 --> BF16, shape = {24576, 3072}\n",
      "INFO:hf-to-gguf:blk.25.ffn_gate.weight,    torch.bfloat16 --> BF16, shape = {3072, 24576}\n",
      "INFO:hf-to-gguf:blk.25.ffn_up.weight,      torch.bfloat16 --> BF16, shape = {3072, 24576}\n",
      "INFO:hf-to-gguf:blk.25.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.25.attn_k.weight,      torch.bfloat16 --> BF16, shape = {3072, 4096}\n",
      "INFO:hf-to-gguf:blk.25.attn_output.weight, torch.bfloat16 --> BF16, shape = {4096, 3072}\n",
      "INFO:hf-to-gguf:blk.25.attn_q.weight,      torch.bfloat16 --> BF16, shape = {3072, 4096}\n",
      "INFO:hf-to-gguf:blk.25.attn_v.weight,      torch.bfloat16 --> BF16, shape = {3072, 4096}\n",
      "INFO:hf-to-gguf:blk.26.attn_norm.weight,   torch.bfloat16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.26.ffn_down.weight,    torch.bfloat16 --> BF16, shape = {24576, 3072}\n",
      "INFO:hf-to-gguf:blk.26.ffn_gate.weight,    torch.bfloat16 --> BF16, shape = {3072, 24576}\n",
      "INFO:hf-to-gguf:blk.26.ffn_up.weight,      torch.bfloat16 --> BF16, shape = {3072, 24576}\n",
      "INFO:hf-to-gguf:blk.26.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.26.attn_k.weight,      torch.bfloat16 --> BF16, shape = {3072, 4096}\n",
      "INFO:hf-to-gguf:blk.26.attn_output.weight, torch.bfloat16 --> BF16, shape = {4096, 3072}\n",
      "INFO:hf-to-gguf:blk.26.attn_q.weight,      torch.bfloat16 --> BF16, shape = {3072, 4096}\n",
      "INFO:hf-to-gguf:blk.26.attn_v.weight,      torch.bfloat16 --> BF16, shape = {3072, 4096}\n",
      "INFO:hf-to-gguf:blk.27.attn_norm.weight,   torch.bfloat16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.27.ffn_down.weight,    torch.bfloat16 --> BF16, shape = {24576, 3072}\n",
      "INFO:hf-to-gguf:blk.27.ffn_gate.weight,    torch.bfloat16 --> BF16, shape = {3072, 24576}\n",
      "INFO:hf-to-gguf:blk.27.ffn_up.weight,      torch.bfloat16 --> BF16, shape = {3072, 24576}\n",
      "INFO:hf-to-gguf:blk.27.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:blk.27.attn_k.weight,      torch.bfloat16 --> BF16, shape = {3072, 4096}\n",
      "INFO:hf-to-gguf:blk.27.attn_output.weight, torch.bfloat16 --> BF16, shape = {4096, 3072}\n",
      "INFO:hf-to-gguf:blk.27.attn_q.weight,      torch.bfloat16 --> BF16, shape = {3072, 4096}\n",
      "INFO:hf-to-gguf:blk.27.attn_v.weight,      torch.bfloat16 --> BF16, shape = {3072, 4096}\n",
      "INFO:hf-to-gguf:output_norm.weight,        torch.bfloat16 --> F32, shape = {3072}\n",
      "INFO:hf-to-gguf:Set meta model\n",
      "INFO:hf-to-gguf:Set model parameters\n",
      "INFO:hf-to-gguf:Set model tokenizer\n",
      "INFO:gguf.vocab:Setting special token type bos to 2\n",
      "INFO:gguf.vocab:Setting special token type eos to 1\n",
      "INFO:gguf.vocab:Setting special token type unk to 3\n",
      "INFO:gguf.vocab:Setting special token type pad to 0\n",
      "INFO:gguf.vocab:Setting add_bos_token to True\n",
      "INFO:gguf.vocab:Setting add_eos_token to False\n",
      "WARNING:gguf.vocab:No handler for special token type prefix with id 67 - skipping\n",
      "WARNING:gguf.vocab:No handler for special token type suffix with id 69 - skipping\n",
      "WARNING:gguf.vocab:No handler for special token type middle with id 68 - skipping\n",
      "WARNING:gguf.vocab:No handler for special token type fsep with id 70 - skipping\n",
      "INFO:gguf.vocab:Setting special token type eot to 107\n",
      "INFO:hf-to-gguf:Set model quantization version\n",
      "INFO:gguf.gguf_writer:Writing the following files:\n",
      "INFO:gguf.gguf_writer:/teamspace/studios/this_studio/stock_advisor/unsloth.BF16.gguf: n_tensors = 254, total_size = 17.1G\n",
      "Writing: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 17.1G/17.1G [01:12<00:00, 236Mbyte/s]\n",
      "INFO:hf-to-gguf:Model successfully exported to /teamspace/studios/this_studio/stock_advisor/unsloth.BF16.gguf\n",
      "Unsloth: Conversion completed! Output location: /teamspace/studios/this_studio/stock_advisor/unsloth.BF16.gguf\n",
      "Unsloth: [2] Converting GGUF 16bit into q4_k_m. This will take 20 minutes...\n",
      "main: build = 3982 (cc2983d3)\n",
      "main: built with cc (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0 for x86_64-linux-gnu\n",
      "main: quantizing '/teamspace/studios/this_studio/stock_advisor/unsloth.BF16.gguf' to '/teamspace/studios/this_studio/stock_advisor/unsloth.Q4_K_M.gguf' as Q4_K_M using 64 threads\n",
      "llama_model_loader: loaded meta data with 31 key-value pairs and 254 tensors from /teamspace/studios/this_studio/stock_advisor/unsloth.BF16.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = gemma\n",
      "llama_model_loader: - kv   1:                               general.type str              = model\n",
      "llama_model_loader: - kv   2:                               general.name str              = Gemma 7b Bnb 4bit\n",
      "llama_model_loader: - kv   3:                       general.organization str              = Unsloth\n",
      "llama_model_loader: - kv   4:                           general.finetune str              = bnb-4bit\n",
      "llama_model_loader: - kv   5:                           general.basename str              = gemma\n",
      "llama_model_loader: - kv   6:                         general.size_label str              = 7B\n",
      "llama_model_loader: - kv   7:                       gemma.context_length u32              = 8192\n",
      "llama_model_loader: - kv   8:                     gemma.embedding_length u32              = 3072\n",
      "llama_model_loader: - kv   9:                          gemma.block_count u32              = 28\n",
      "llama_model_loader: - kv  10:                  gemma.feed_forward_length u32              = 24576\n",
      "llama_model_loader: - kv  11:                 gemma.attention.head_count u32              = 16\n",
      "llama_model_loader: - kv  12:              gemma.attention.head_count_kv u32              = 16\n",
      "llama_model_loader: - kv  13:     gemma.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv  14:                 gemma.attention.key_length u32              = 256\n",
      "llama_model_loader: - kv  15:               gemma.attention.value_length u32              = 256\n",
      "llama_model_loader: - kv  16:                          general.file_type u32              = 32\n",
      "llama_model_loader: - kv  17:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  18:                         tokenizer.ggml.pre str              = default\n",
      "llama_model_loader: - kv  19:                      tokenizer.ggml.tokens arr[str,256000]  = [\"<pad>\", \"<eos>\", \"<bos>\", \"<unk>\", ...\n",
      "llama_model_loader: - kv  20:                      tokenizer.ggml.scores arr[f32,256000]  = [-1000.000000, -1000.000000, -1000.00...\n",
      "llama_model_loader: - kv  21:                  tokenizer.ggml.token_type arr[i32,256000]  = [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, ...\n",
      "llama_model_loader: - kv  22:                tokenizer.ggml.bos_token_id u32              = 2\n",
      "llama_model_loader: - kv  23:                tokenizer.ggml.eos_token_id u32              = 1\n",
      "llama_model_loader: - kv  24:            tokenizer.ggml.unknown_token_id u32              = 3\n",
      "llama_model_loader: - kv  25:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  26:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  27:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  28:                tokenizer.ggml.eot_token_id u32              = 107\n",
      "llama_model_loader: - kv  29:            tokenizer.ggml.add_space_prefix bool             = false\n",
      "llama_model_loader: - kv  30:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   57 tensors\n",
      "llama_model_loader: - type bf16:  197 tensors\n",
      "[   1/ 254]                    token_embd.weight - [ 3072, 256000,     1,     1], type =   bf16, converting to q6_K .. size =  1500.00 MiB ->   615.23 MiB\n",
      "[   2/ 254]               blk.0.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[   3/ 254]                blk.0.ffn_down.weight - [24576,  3072,     1,     1], type =   bf16, converting to q6_K .. size =   144.00 MiB ->    59.06 MiB\n",
      "[   4/ 254]                blk.0.ffn_gate.weight - [ 3072, 24576,     1,     1], type =   bf16, converting to q4_K .. size =   144.00 MiB ->    40.50 MiB\n",
      "[   5/ 254]                  blk.0.ffn_up.weight - [ 3072, 24576,     1,     1], type =   bf16, converting to q4_K .. size =   144.00 MiB ->    40.50 MiB\n",
      "[   6/ 254]                blk.0.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[   7/ 254]                  blk.0.attn_k.weight - [ 3072,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB\n",
      "[   8/ 254]             blk.0.attn_output.weight - [ 4096,  3072,     1,     1], type =   bf16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB\n",
      "[   9/ 254]                  blk.0.attn_q.weight - [ 3072,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB\n",
      "[  10/ 254]                  blk.0.attn_v.weight - [ 3072,  4096,     1,     1], type =   bf16, converting to q6_K .. size =    24.00 MiB ->     9.84 MiB\n",
      "[  11/ 254]               blk.1.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[  12/ 254]                blk.1.ffn_down.weight - [24576,  3072,     1,     1], type =   bf16, converting to q6_K .. size =   144.00 MiB ->    59.06 MiB\n",
      "[  13/ 254]                blk.1.ffn_gate.weight - [ 3072, 24576,     1,     1], type =   bf16, converting to q4_K .. size =   144.00 MiB ->    40.50 MiB\n",
      "[  14/ 254]                  blk.1.ffn_up.weight - [ 3072, 24576,     1,     1], type =   bf16, converting to q4_K .. size =   144.00 MiB ->    40.50 MiB\n",
      "[  15/ 254]                blk.1.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[  16/ 254]                  blk.1.attn_k.weight - [ 3072,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB\n",
      "[  17/ 254]             blk.1.attn_output.weight - [ 4096,  3072,     1,     1], type =   bf16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB\n",
      "[  18/ 254]                  blk.1.attn_q.weight - [ 3072,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB\n",
      "[  19/ 254]                  blk.1.attn_v.weight - [ 3072,  4096,     1,     1], type =   bf16, converting to q6_K .. size =    24.00 MiB ->     9.84 MiB\n",
      "[  20/ 254]               blk.2.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[  21/ 254]                blk.2.ffn_down.weight - [24576,  3072,     1,     1], type =   bf16, converting to q6_K .. size =   144.00 MiB ->    59.06 MiB\n",
      "[  22/ 254]                blk.2.ffn_gate.weight - [ 3072, 24576,     1,     1], type =   bf16, converting to q4_K .. size =   144.00 MiB ->    40.50 MiB\n",
      "[  23/ 254]                  blk.2.ffn_up.weight - [ 3072, 24576,     1,     1], type =   bf16, converting to q4_K .. size =   144.00 MiB ->    40.50 MiB\n",
      "[  24/ 254]                blk.2.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[  25/ 254]                  blk.2.attn_k.weight - [ 3072,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB\n",
      "[  26/ 254]             blk.2.attn_output.weight - [ 4096,  3072,     1,     1], type =   bf16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB\n",
      "[  27/ 254]                  blk.2.attn_q.weight - [ 3072,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB\n",
      "[  28/ 254]                  blk.2.attn_v.weight - [ 3072,  4096,     1,     1], type =   bf16, converting to q6_K .. size =    24.00 MiB ->     9.84 MiB\n",
      "[  29/ 254]               blk.3.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[  30/ 254]                blk.3.ffn_down.weight - [24576,  3072,     1,     1], type =   bf16, converting to q4_K .. size =   144.00 MiB ->    40.50 MiB\n",
      "[  31/ 254]                blk.3.ffn_gate.weight - [ 3072, 24576,     1,     1], type =   bf16, converting to q4_K .. size =   144.00 MiB ->    40.50 MiB\n",
      "[  32/ 254]                  blk.3.ffn_up.weight - [ 3072, 24576,     1,     1], type =   bf16, converting to q4_K .. size =   144.00 MiB ->    40.50 MiB\n",
      "[  33/ 254]                blk.3.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[  34/ 254]                  blk.3.attn_k.weight - [ 3072,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB\n",
      "[  35/ 254]             blk.3.attn_output.weight - [ 4096,  3072,     1,     1], type =   bf16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB\n",
      "[  36/ 254]                  blk.3.attn_q.weight - [ 3072,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB\n",
      "[  37/ 254]                  blk.3.attn_v.weight - [ 3072,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB\n",
      "[  38/ 254]               blk.4.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[  39/ 254]                blk.4.ffn_down.weight - [24576,  3072,     1,     1], type =   bf16, converting to q4_K .. size =   144.00 MiB ->    40.50 MiB\n",
      "[  40/ 254]                blk.4.ffn_gate.weight - [ 3072, 24576,     1,     1], type =   bf16, converting to q4_K .. size =   144.00 MiB ->    40.50 MiB\n",
      "[  41/ 254]                  blk.4.ffn_up.weight - [ 3072, 24576,     1,     1], type =   bf16, converting to q4_K .. size =   144.00 MiB ->    40.50 MiB\n",
      "[  42/ 254]                blk.4.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[  43/ 254]                  blk.4.attn_k.weight - [ 3072,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB\n",
      "[  44/ 254]             blk.4.attn_output.weight - [ 4096,  3072,     1,     1], type =   bf16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB\n",
      "[  45/ 254]                  blk.4.attn_q.weight - [ 3072,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB\n",
      "[  46/ 254]                  blk.4.attn_v.weight - [ 3072,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB\n",
      "[  47/ 254]               blk.5.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[  48/ 254]                blk.5.ffn_down.weight - [24576,  3072,     1,     1], type =   bf16, converting to q6_K .. size =   144.00 MiB ->    59.06 MiB\n",
      "[  49/ 254]                blk.5.ffn_gate.weight - [ 3072, 24576,     1,     1], type =   bf16, converting to q4_K .. size =   144.00 MiB ->    40.50 MiB\n",
      "[  50/ 254]                  blk.5.ffn_up.weight - [ 3072, 24576,     1,     1], type =   bf16, converting to q4_K .. size =   144.00 MiB ->    40.50 MiB\n",
      "[  51/ 254]                blk.5.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[  52/ 254]                  blk.5.attn_k.weight - [ 3072,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB\n",
      "[  53/ 254]             blk.5.attn_output.weight - [ 4096,  3072,     1,     1], type =   bf16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB\n",
      "[  54/ 254]                  blk.5.attn_q.weight - [ 3072,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB\n",
      "[  55/ 254]                  blk.5.attn_v.weight - [ 3072,  4096,     1,     1], type =   bf16, converting to q6_K .. size =    24.00 MiB ->     9.84 MiB\n",
      "[  56/ 254]                  blk.6.attn_k.weight - [ 3072,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB\n",
      "[  57/ 254]             blk.6.attn_output.weight - [ 4096,  3072,     1,     1], type =   bf16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB\n",
      "[  58/ 254]                  blk.6.attn_q.weight - [ 3072,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB\n",
      "[  59/ 254]                  blk.6.attn_v.weight - [ 3072,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB\n",
      "[  60/ 254]              blk.10.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[  61/ 254]               blk.10.ffn_down.weight - [24576,  3072,     1,     1], type =   bf16, converting to q4_K .. size =   144.00 MiB ->    40.50 MiB\n",
      "[  62/ 254]               blk.10.ffn_gate.weight - [ 3072, 24576,     1,     1], type =   bf16, converting to q4_K .. size =   144.00 MiB ->    40.50 MiB\n",
      "[  63/ 254]                 blk.10.ffn_up.weight - [ 3072, 24576,     1,     1], type =   bf16, converting to q4_K .. size =   144.00 MiB ->    40.50 MiB\n",
      "[  64/ 254]               blk.10.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[  65/ 254]                 blk.10.attn_k.weight - [ 3072,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB\n",
      "[  66/ 254]            blk.10.attn_output.weight - [ 4096,  3072,     1,     1], type =   bf16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB\n",
      "[  67/ 254]                 blk.10.attn_q.weight - [ 3072,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB\n",
      "[  68/ 254]                 blk.10.attn_v.weight - [ 3072,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB\n",
      "[  69/ 254]              blk.11.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[  70/ 254]               blk.11.ffn_down.weight - [24576,  3072,     1,     1], type =   bf16, converting to q4_K .. size =   144.00 MiB ->    40.50 MiB\n",
      "[  71/ 254]               blk.11.ffn_gate.weight - [ 3072, 24576,     1,     1], type =   bf16, converting to q4_K .. size =   144.00 MiB ->    40.50 MiB\n",
      "[  72/ 254]                 blk.11.ffn_up.weight - [ 3072, 24576,     1,     1], type =   bf16, converting to q4_K .. size =   144.00 MiB ->    40.50 MiB\n",
      "[  73/ 254]               blk.11.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[  74/ 254]                 blk.11.attn_k.weight - [ 3072,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB\n",
      "[  75/ 254]            blk.11.attn_output.weight - [ 4096,  3072,     1,     1], type =   bf16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB\n",
      "[  76/ 254]                 blk.11.attn_q.weight - [ 3072,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB\n",
      "[  77/ 254]                 blk.11.attn_v.weight - [ 3072,  4096,     1,     1], type =   bf16, converting to q6_K .. size =    24.00 MiB ->     9.84 MiB\n",
      "[  78/ 254]              blk.12.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[  79/ 254]               blk.12.ffn_down.weight - [24576,  3072,     1,     1], type =   bf16, converting to q6_K .. size =   144.00 MiB ->    59.06 MiB\n",
      "[  80/ 254]               blk.12.ffn_gate.weight - [ 3072, 24576,     1,     1], type =   bf16, converting to q4_K .. size =   144.00 MiB ->    40.50 MiB\n",
      "[  81/ 254]                 blk.12.ffn_up.weight - [ 3072, 24576,     1,     1], type =   bf16, converting to q4_K .. size =   144.00 MiB ->    40.50 MiB\n",
      "[  82/ 254]               blk.12.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[  83/ 254]                 blk.12.attn_k.weight - [ 3072,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB\n",
      "[  84/ 254]            blk.12.attn_output.weight - [ 4096,  3072,     1,     1], type =   bf16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB\n",
      "[  85/ 254]                 blk.12.attn_q.weight - [ 3072,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB\n",
      "[  86/ 254]                 blk.12.attn_v.weight - [ 3072,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB\n",
      "[  87/ 254]              blk.13.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[  88/ 254]               blk.13.ffn_down.weight - [24576,  3072,     1,     1], type =   bf16, converting to q4_K .. size =   144.00 MiB ->    40.50 MiB\n",
      "[  89/ 254]               blk.13.ffn_gate.weight - [ 3072, 24576,     1,     1], type =   bf16, converting to q4_K .. size =   144.00 MiB ->    40.50 MiB\n",
      "[  90/ 254]                 blk.13.ffn_up.weight - [ 3072, 24576,     1,     1], type =   bf16, converting to q4_K .. size =   144.00 MiB ->    40.50 MiB\n",
      "[  91/ 254]               blk.13.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[  92/ 254]                 blk.13.attn_k.weight - [ 3072,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB\n",
      "[  93/ 254]            blk.13.attn_output.weight - [ 4096,  3072,     1,     1], type =   bf16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB\n",
      "[  94/ 254]                 blk.13.attn_q.weight - [ 3072,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB\n",
      "[  95/ 254]                 blk.13.attn_v.weight - [ 3072,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB\n",
      "[  96/ 254]              blk.14.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[  97/ 254]               blk.14.ffn_down.weight - [24576,  3072,     1,     1], type =   bf16, converting to q4_K .. size =   144.00 MiB ->    40.50 MiB\n",
      "[  98/ 254]               blk.14.ffn_gate.weight - [ 3072, 24576,     1,     1], type =   bf16, converting to q4_K .. size =   144.00 MiB ->    40.50 MiB\n",
      "[  99/ 254]                 blk.14.ffn_up.weight - [ 3072, 24576,     1,     1], type =   bf16, converting to q4_K .. size =   144.00 MiB ->    40.50 MiB\n",
      "[ 100/ 254]               blk.14.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 101/ 254]                 blk.14.attn_k.weight - [ 3072,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB\n",
      "[ 102/ 254]            blk.14.attn_output.weight - [ 4096,  3072,     1,     1], type =   bf16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB\n",
      "[ 103/ 254]                 blk.14.attn_q.weight - [ 3072,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB\n",
      "[ 104/ 254]                 blk.14.attn_v.weight - [ 3072,  4096,     1,     1], type =   bf16, converting to q6_K .. size =    24.00 MiB ->     9.84 MiB\n",
      "[ 105/ 254]                 blk.15.attn_k.weight - [ 3072,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB\n",
      "[ 106/ 254]            blk.15.attn_output.weight - [ 4096,  3072,     1,     1], type =   bf16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB\n",
      "[ 107/ 254]                 blk.15.attn_q.weight - [ 3072,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB\n",
      "[ 108/ 254]                 blk.15.attn_v.weight - [ 3072,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB\n",
      "[ 109/ 254]               blk.6.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 110/ 254]                blk.6.ffn_down.weight - [24576,  3072,     1,     1], type =   bf16, converting to q6_K .. size =   144.00 MiB ->    59.06 MiB\n",
      "[ 111/ 254]                blk.6.ffn_gate.weight - [ 3072, 24576,     1,     1], type =   bf16, converting to q4_K .. size =   144.00 MiB ->    40.50 MiB\n",
      "[ 112/ 254]                  blk.6.ffn_up.weight - [ 3072, 24576,     1,     1], type =   bf16, converting to q4_K .. size =   144.00 MiB ->    40.50 MiB\n",
      "[ 113/ 254]                blk.6.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 114/ 254]               blk.7.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 115/ 254]                blk.7.ffn_down.weight - [24576,  3072,     1,     1], type =   bf16, converting to q4_K .. size =   144.00 MiB ->    40.50 MiB\n",
      "[ 116/ 254]                blk.7.ffn_gate.weight - [ 3072, 24576,     1,     1], type =   bf16, converting to q4_K .. size =   144.00 MiB ->    40.50 MiB\n",
      "[ 117/ 254]                  blk.7.ffn_up.weight - [ 3072, 24576,     1,     1], type =   bf16, converting to q4_K .. size =   144.00 MiB ->    40.50 MiB\n",
      "[ 118/ 254]                blk.7.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 119/ 254]                  blk.7.attn_k.weight - [ 3072,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB\n",
      "[ 120/ 254]             blk.7.attn_output.weight - [ 4096,  3072,     1,     1], type =   bf16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB\n",
      "[ 121/ 254]                  blk.7.attn_q.weight - [ 3072,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB\n",
      "[ 122/ 254]                  blk.7.attn_v.weight - [ 3072,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB\n",
      "[ 123/ 254]               blk.8.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 124/ 254]                blk.8.ffn_down.weight - [24576,  3072,     1,     1], type =   bf16, converting to q4_K .. size =   144.00 MiB ->    40.50 MiB\n",
      "[ 125/ 254]                blk.8.ffn_gate.weight - [ 3072, 24576,     1,     1], type =   bf16, converting to q4_K .. size =   144.00 MiB ->    40.50 MiB\n",
      "[ 126/ 254]                  blk.8.ffn_up.weight - [ 3072, 24576,     1,     1], type =   bf16, converting to q4_K .. size =   144.00 MiB ->    40.50 MiB\n",
      "[ 127/ 254]                blk.8.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 128/ 254]                  blk.8.attn_k.weight - [ 3072,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB\n",
      "[ 129/ 254]             blk.8.attn_output.weight - [ 4096,  3072,     1,     1], type =   bf16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB\n",
      "[ 130/ 254]                  blk.8.attn_q.weight - [ 3072,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB\n",
      "[ 131/ 254]                  blk.8.attn_v.weight - [ 3072,  4096,     1,     1], type =   bf16, converting to q6_K .. size =    24.00 MiB ->     9.84 MiB\n",
      "[ 132/ 254]               blk.9.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 133/ 254]                blk.9.ffn_down.weight - [24576,  3072,     1,     1], type =   bf16, converting to q6_K .. size =   144.00 MiB ->    59.06 MiB\n",
      "[ 134/ 254]                blk.9.ffn_gate.weight - [ 3072, 24576,     1,     1], type =   bf16, converting to q4_K .. size =   144.00 MiB ->    40.50 MiB\n",
      "[ 135/ 254]                  blk.9.ffn_up.weight - [ 3072, 24576,     1,     1], type =   bf16, converting to q4_K .. size =   144.00 MiB ->    40.50 MiB\n",
      "[ 136/ 254]                blk.9.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 137/ 254]                  blk.9.attn_k.weight - [ 3072,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB\n",
      "[ 138/ 254]             blk.9.attn_output.weight - [ 4096,  3072,     1,     1], type =   bf16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB\n",
      "[ 139/ 254]                  blk.9.attn_q.weight - [ 3072,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB\n",
      "[ 140/ 254]                  blk.9.attn_v.weight - [ 3072,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB\n",
      "[ 141/ 254]              blk.15.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 142/ 254]               blk.15.ffn_down.weight - [24576,  3072,     1,     1], type =   bf16, converting to q4_K .. size =   144.00 MiB ->    40.50 MiB\n",
      "[ 143/ 254]               blk.15.ffn_gate.weight - [ 3072, 24576,     1,     1], type =   bf16, converting to q4_K .. size =   144.00 MiB ->    40.50 MiB\n",
      "[ 144/ 254]                 blk.15.ffn_up.weight - [ 3072, 24576,     1,     1], type =   bf16, converting to q4_K .. size =   144.00 MiB ->    40.50 MiB\n",
      "[ 145/ 254]               blk.15.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 146/ 254]              blk.16.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 147/ 254]               blk.16.ffn_down.weight - [24576,  3072,     1,     1], type =   bf16, converting to q4_K .. size =   144.00 MiB ->    40.50 MiB\n",
      "[ 148/ 254]               blk.16.ffn_gate.weight - [ 3072, 24576,     1,     1], type =   bf16, converting to q4_K .. size =   144.00 MiB ->    40.50 MiB\n",
      "[ 149/ 254]                 blk.16.ffn_up.weight - [ 3072, 24576,     1,     1], type =   bf16, converting to q4_K .. size =   144.00 MiB ->    40.50 MiB\n",
      "[ 150/ 254]               blk.16.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 151/ 254]                 blk.16.attn_k.weight - [ 3072,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB\n",
      "[ 152/ 254]            blk.16.attn_output.weight - [ 4096,  3072,     1,     1], type =   bf16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB\n",
      "[ 153/ 254]                 blk.16.attn_q.weight - [ 3072,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB\n",
      "[ 154/ 254]                 blk.16.attn_v.weight - [ 3072,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB\n",
      "[ 155/ 254]              blk.17.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 156/ 254]               blk.17.ffn_down.weight - [24576,  3072,     1,     1], type =   bf16, converting to q6_K .. size =   144.00 MiB ->    59.06 MiB\n",
      "[ 157/ 254]               blk.17.ffn_gate.weight - [ 3072, 24576,     1,     1], type =   bf16, converting to q4_K .. size =   144.00 MiB ->    40.50 MiB\n",
      "[ 158/ 254]                 blk.17.ffn_up.weight - [ 3072, 24576,     1,     1], type =   bf16, converting to q4_K .. size =   144.00 MiB ->    40.50 MiB\n",
      "[ 159/ 254]               blk.17.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 160/ 254]                 blk.17.attn_k.weight - [ 3072,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB\n",
      "[ 161/ 254]            blk.17.attn_output.weight - [ 4096,  3072,     1,     1], type =   bf16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB\n",
      "[ 162/ 254]                 blk.17.attn_q.weight - [ 3072,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB\n",
      "[ 163/ 254]                 blk.17.attn_v.weight - [ 3072,  4096,     1,     1], type =   bf16, converting to q6_K .. size =    24.00 MiB ->     9.84 MiB\n",
      "[ 164/ 254]              blk.18.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 165/ 254]               blk.18.ffn_down.weight - [24576,  3072,     1,     1], type =   bf16, converting to q4_K .. size =   144.00 MiB ->    40.50 MiB\n",
      "[ 166/ 254]               blk.18.ffn_gate.weight - [ 3072, 24576,     1,     1], type =   bf16, converting to q4_K .. size =   144.00 MiB ->    40.50 MiB\n",
      "[ 167/ 254]                 blk.18.ffn_up.weight - [ 3072, 24576,     1,     1], type =   bf16, converting to q4_K .. size =   144.00 MiB ->    40.50 MiB\n",
      "[ 168/ 254]               blk.18.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 169/ 254]                 blk.18.attn_k.weight - [ 3072,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB\n",
      "[ 170/ 254]            blk.18.attn_output.weight - [ 4096,  3072,     1,     1], type =   bf16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB\n",
      "[ 171/ 254]                 blk.18.attn_q.weight - [ 3072,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB\n",
      "[ 172/ 254]                 blk.18.attn_v.weight - [ 3072,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB\n",
      "[ 173/ 254]              blk.19.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 174/ 254]               blk.19.ffn_down.weight - [24576,  3072,     1,     1], type =   bf16, converting to q4_K .. size =   144.00 MiB ->    40.50 MiB\n",
      "[ 175/ 254]               blk.19.ffn_gate.weight - [ 3072, 24576,     1,     1], type =   bf16, converting to q4_K .. size =   144.00 MiB ->    40.50 MiB\n",
      "[ 176/ 254]                 blk.19.ffn_up.weight - [ 3072, 24576,     1,     1], type =   bf16, converting to q4_K .. size =   144.00 MiB ->    40.50 MiB\n",
      "[ 177/ 254]               blk.19.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 178/ 254]                 blk.19.attn_k.weight - [ 3072,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB\n",
      "[ 179/ 254]            blk.19.attn_output.weight - [ 4096,  3072,     1,     1], type =   bf16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB\n",
      "[ 180/ 254]                 blk.19.attn_q.weight - [ 3072,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB\n",
      "[ 181/ 254]                 blk.19.attn_v.weight - [ 3072,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB\n",
      "[ 182/ 254]              blk.20.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 183/ 254]               blk.20.ffn_down.weight - [24576,  3072,     1,     1], type =   bf16, converting to q6_K .. size =   144.00 MiB ->    59.06 MiB\n",
      "[ 184/ 254]               blk.20.ffn_gate.weight - [ 3072, 24576,     1,     1], type =   bf16, converting to q4_K .. size =   144.00 MiB ->    40.50 MiB\n",
      "[ 185/ 254]                 blk.20.ffn_up.weight - [ 3072, 24576,     1,     1], type =   bf16, converting to q4_K .. size =   144.00 MiB ->    40.50 MiB\n",
      "[ 186/ 254]               blk.20.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 187/ 254]                 blk.20.attn_k.weight - [ 3072,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB\n",
      "[ 188/ 254]            blk.20.attn_output.weight - [ 4096,  3072,     1,     1], type =   bf16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB\n",
      "[ 189/ 254]                 blk.20.attn_q.weight - [ 3072,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB\n",
      "[ 190/ 254]                 blk.20.attn_v.weight - [ 3072,  4096,     1,     1], type =   bf16, converting to q6_K .. size =    24.00 MiB ->     9.84 MiB\n",
      "[ 191/ 254]              blk.21.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 192/ 254]               blk.21.ffn_down.weight - [24576,  3072,     1,     1], type =   bf16, converting to q4_K .. size =   144.00 MiB ->    40.50 MiB\n",
      "[ 193/ 254]               blk.21.ffn_gate.weight - [ 3072, 24576,     1,     1], type =   bf16, converting to q4_K .. size =   144.00 MiB ->    40.50 MiB\n",
      "[ 194/ 254]                 blk.21.ffn_up.weight - [ 3072, 24576,     1,     1], type =   bf16, converting to q4_K .. size =   144.00 MiB ->    40.50 MiB\n",
      "[ 195/ 254]               blk.21.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 196/ 254]                 blk.21.attn_k.weight - [ 3072,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB\n",
      "[ 197/ 254]            blk.21.attn_output.weight - [ 4096,  3072,     1,     1], type =   bf16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB\n",
      "[ 198/ 254]                 blk.21.attn_q.weight - [ 3072,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB\n",
      "[ 199/ 254]                 blk.21.attn_v.weight - [ 3072,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB\n",
      "[ 200/ 254]              blk.22.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 201/ 254]               blk.22.ffn_down.weight - [24576,  3072,     1,     1], type =   bf16, converting to q4_K .. size =   144.00 MiB ->    40.50 MiB\n",
      "[ 202/ 254]               blk.22.ffn_gate.weight - [ 3072, 24576,     1,     1], type =   bf16, converting to q4_K .. size =   144.00 MiB ->    40.50 MiB\n",
      "[ 203/ 254]                 blk.22.ffn_up.weight - [ 3072, 24576,     1,     1], type =   bf16, converting to q4_K .. size =   144.00 MiB ->    40.50 MiB\n",
      "[ 204/ 254]               blk.22.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 205/ 254]                 blk.22.attn_k.weight - [ 3072,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB\n",
      "[ 206/ 254]            blk.22.attn_output.weight - [ 4096,  3072,     1,     1], type =   bf16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB\n",
      "[ 207/ 254]                 blk.22.attn_q.weight - [ 3072,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB\n",
      "[ 208/ 254]                 blk.22.attn_v.weight - [ 3072,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB\n",
      "[ 209/ 254]              blk.23.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 210/ 254]               blk.23.ffn_down.weight - [24576,  3072,     1,     1], type =   bf16, converting to q6_K .. size =   144.00 MiB ->    59.06 MiB\n",
      "[ 211/ 254]               blk.23.ffn_gate.weight - [ 3072, 24576,     1,     1], type =   bf16, converting to q4_K .. size =   144.00 MiB ->    40.50 MiB\n",
      "[ 212/ 254]                 blk.23.ffn_up.weight - [ 3072, 24576,     1,     1], type =   bf16, converting to q4_K .. size =   144.00 MiB ->    40.50 MiB\n",
      "[ 213/ 254]               blk.23.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 214/ 254]                 blk.23.attn_k.weight - [ 3072,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB\n",
      "[ 215/ 254]            blk.23.attn_output.weight - [ 4096,  3072,     1,     1], type =   bf16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB\n",
      "[ 216/ 254]                 blk.23.attn_q.weight - [ 3072,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB\n",
      "[ 217/ 254]                 blk.23.attn_v.weight - [ 3072,  4096,     1,     1], type =   bf16, converting to q6_K .. size =    24.00 MiB ->     9.84 MiB\n",
      "[ 218/ 254]                 blk.24.attn_k.weight - [ 3072,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB\n",
      "[ 219/ 254]            blk.24.attn_output.weight - [ 4096,  3072,     1,     1], type =   bf16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB\n",
      "[ 220/ 254]                 blk.24.attn_q.weight - [ 3072,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB\n",
      "[ 221/ 254]                 blk.24.attn_v.weight - [ 3072,  4096,     1,     1], type =   bf16, converting to q6_K .. size =    24.00 MiB ->     9.84 MiB\n",
      "[ 222/ 254]              blk.24.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 223/ 254]               blk.24.ffn_down.weight - [24576,  3072,     1,     1], type =   bf16, converting to q6_K .. size =   144.00 MiB ->    59.06 MiB\n",
      "[ 224/ 254]               blk.24.ffn_gate.weight - [ 3072, 24576,     1,     1], type =   bf16, converting to q4_K .. size =   144.00 MiB ->    40.50 MiB\n",
      "[ 225/ 254]                 blk.24.ffn_up.weight - [ 3072, 24576,     1,     1], type =   bf16, converting to q4_K .. size =   144.00 MiB ->    40.50 MiB\n",
      "[ 226/ 254]               blk.24.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 227/ 254]              blk.25.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 228/ 254]               blk.25.ffn_down.weight - [24576,  3072,     1,     1], type =   bf16, converting to q6_K .. size =   144.00 MiB ->    59.06 MiB\n",
      "[ 229/ 254]               blk.25.ffn_gate.weight - [ 3072, 24576,     1,     1], type =   bf16, converting to q4_K .. size =   144.00 MiB ->    40.50 MiB\n",
      "[ 230/ 254]                 blk.25.ffn_up.weight - [ 3072, 24576,     1,     1], type =   bf16, converting to q4_K .. size =   144.00 MiB ->    40.50 MiB\n",
      "[ 231/ 254]               blk.25.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 232/ 254]                 blk.25.attn_k.weight - [ 3072,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB\n",
      "[ 233/ 254]            blk.25.attn_output.weight - [ 4096,  3072,     1,     1], type =   bf16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB\n",
      "[ 234/ 254]                 blk.25.attn_q.weight - [ 3072,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB\n",
      "[ 235/ 254]                 blk.25.attn_v.weight - [ 3072,  4096,     1,     1], type =   bf16, converting to q6_K .. size =    24.00 MiB ->     9.84 MiB\n",
      "[ 236/ 254]              blk.26.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 237/ 254]               blk.26.ffn_down.weight - [24576,  3072,     1,     1], type =   bf16, converting to q6_K .. size =   144.00 MiB ->    59.06 MiB\n",
      "[ 238/ 254]               blk.26.ffn_gate.weight - [ 3072, 24576,     1,     1], type =   bf16, converting to q4_K .. size =   144.00 MiB ->    40.50 MiB\n",
      "[ 239/ 254]                 blk.26.ffn_up.weight - [ 3072, 24576,     1,     1], type =   bf16, converting to q4_K .. size =   144.00 MiB ->    40.50 MiB\n",
      "[ 240/ 254]               blk.26.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 241/ 254]                 blk.26.attn_k.weight - [ 3072,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB\n",
      "[ 242/ 254]            blk.26.attn_output.weight - [ 4096,  3072,     1,     1], type =   bf16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB\n",
      "[ 243/ 254]                 blk.26.attn_q.weight - [ 3072,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB\n",
      "[ 244/ 254]                 blk.26.attn_v.weight - [ 3072,  4096,     1,     1], type =   bf16, converting to q6_K .. size =    24.00 MiB ->     9.84 MiB\n",
      "[ 245/ 254]              blk.27.attn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 246/ 254]               blk.27.ffn_down.weight - [24576,  3072,     1,     1], type =   bf16, converting to q6_K .. size =   144.00 MiB ->    59.06 MiB\n",
      "[ 247/ 254]               blk.27.ffn_gate.weight - [ 3072, 24576,     1,     1], type =   bf16, converting to q4_K .. size =   144.00 MiB ->    40.50 MiB\n",
      "[ 248/ 254]                 blk.27.ffn_up.weight - [ 3072, 24576,     1,     1], type =   bf16, converting to q4_K .. size =   144.00 MiB ->    40.50 MiB\n",
      "[ 249/ 254]               blk.27.ffn_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "[ 250/ 254]                 blk.27.attn_k.weight - [ 3072,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB\n",
      "[ 251/ 254]            blk.27.attn_output.weight - [ 4096,  3072,     1,     1], type =   bf16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB\n",
      "[ 252/ 254]                 blk.27.attn_q.weight - [ 3072,  4096,     1,     1], type =   bf16, converting to q4_K .. size =    24.00 MiB ->     6.75 MiB\n",
      "[ 253/ 254]                 blk.27.attn_v.weight - [ 3072,  4096,     1,     1], type =   bf16, converting to q6_K .. size =    24.00 MiB ->     9.84 MiB\n",
      "[ 254/ 254]                   output_norm.weight - [ 3072,     1,     1,     1], type =    f32, size =    0.012 MB\n",
      "llama_model_quantize_internal: model size  = 16284.67 MB\n",
      "llama_model_quantize_internal: quant size  =  5077.09 MB\n",
      "\n",
      "main: quantize time = 49503.74 ms\n",
      "main:    total time = 49503.74 ms\n",
      "Unsloth: Conversion completed! Output location: /teamspace/studios/this_studio/stock_advisor/unsloth.Q4_K_M.gguf\n"
     ]
    }
   ],
   "source": [
    "# Save to 8bit Q8_0\n",
    "# if False: model.save_pretrained_gguf(\"model\", tokenizer,)\n",
    "# if False: model.push_to_hub_gguf(\"hf/model\", tokenizer, token = \"\")\n",
    "# Save to 16bit GGUF\n",
    "# if False: model.save_pretrained_gguf(\"model\", tokenizer, quantization_method = \"f16\")\n",
    "# if False: model.push_to_hub_gguf(\"hf/model\", tokenizer, quantization_method = \"f16\",\n",
    "# token = \"\")\n",
    "# Save to q4_k_m GGUF\n",
    "model.save_pretrained_gguf(\"stock_advisor\", tokenizer, quantization_method = \"q4_k_m\")\n",
    "# if False: model.push_to_hub_gguf(\"hf/model\", tokenizer, quantization_method = \"q4_k_m\", token = \"\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "1e3ece336fa44d769a78c07d30e2e9bb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "1e5e0c3df4524f46a68ce9779b4b6b3a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1e855f8b44dc4655bb787d9bb28641b8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_31371a7420db4ecc864aa8a676d8c886",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_69289f1f513f4f0e95506ee0093176ee",
      "value": "Generatingâ€‡trainâ€‡split:â€‡"
     }
    },
    "2d14286158e248aa907fa76471c2a0cd": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_1e855f8b44dc4655bb787d9bb28641b8",
       "IPY_MODEL_d60ea0a87cb641d1875b07d826bcbbec",
       "IPY_MODEL_6e40aec31ca741e69228e776a4a03218"
      ],
      "layout": "IPY_MODEL_ace05913441041738f610becaf34a736"
     }
    },
    "31371a7420db4ecc864aa8a676d8c886": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3142cc4a39584e0a8d4df75c127f52df": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "20px"
     }
    },
    "39b9ade04c44443fbfedbe9f2c118100": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "5c595d50a3b14cfb89e8043d5b4c6914": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ecf49b2fd4e74c9d917d2d3c84dd20e6",
      "max": 12580,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_a636be8e9399475fa77c8a18ad55a65c",
      "value": 12580
     }
    },
    "63f39efb08924f108725076201d93e3b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "69289f1f513f4f0e95506ee0093176ee": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "6e40aec31ca741e69228e776a4a03218": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_63f39efb08924f108725076201d93e3b",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_bd6e0933248d4eafa72fbcbd8c1c3eee",
      "value": "â€‡2500/0â€‡[00:00&lt;00:00,â€‡34213.97â€‡examples/s]"
     }
    },
    "755329363b8247299b262543f4cc2b84": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7cfae8c04b1a4fcaa7ac44adf2a85adf": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_d60f2fb4a7494e96af88c1f697846086",
       "IPY_MODEL_5c595d50a3b14cfb89e8043d5b4c6914",
       "IPY_MODEL_cafeda98e01043e0b92db4ce7e059120"
      ],
      "layout": "IPY_MODEL_e4f8701b2a1143ed8c573fa56fb7ef1d"
     }
    },
    "a636be8e9399475fa77c8a18ad55a65c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "ace05913441041738f610becaf34a736": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "bd6e0933248d4eafa72fbcbd8c1c3eee": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "cafeda98e01043e0b92db4ce7e059120": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_755329363b8247299b262543f4cc2b84",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_df7719d65ebb429cb12706602207ef62",
      "value": "â€‡12580/12580â€‡[00:13&lt;00:00,â€‡1367.21â€‡examples/s]"
     }
    },
    "d60ea0a87cb641d1875b07d826bcbbec": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_3142cc4a39584e0a8d4df75c127f52df",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_39b9ade04c44443fbfedbe9f2c118100",
      "value": 1
     }
    },
    "d60f2fb4a7494e96af88c1f697846086": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1e5e0c3df4524f46a68ce9779b4b6b3a",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_1e3ece336fa44d769a78c07d30e2e9bb",
      "value": "Map:â€‡100%"
     }
    },
    "df7719d65ebb429cb12706602207ef62": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "e4f8701b2a1143ed8c573fa56fb7ef1d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ecf49b2fd4e74c9d917d2d3c84dd20e6": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
